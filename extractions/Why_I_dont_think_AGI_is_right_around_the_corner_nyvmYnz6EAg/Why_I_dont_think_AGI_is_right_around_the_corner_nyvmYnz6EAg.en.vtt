WEBVTT
Kind: captions
Language: en

00:00:00.240 --> 00:00:04.880
I’ve had a lot of discussions on my podcast&nbsp;
where we haggle out our timelines to AGI.&nbsp;

00:00:04.880 --> 00:00:08.000
Some guests think it’s 20&nbsp;
years away. Others 2 years.&nbsp;

00:00:08.000 --> 00:00:16.560
Here’s where my thoughts lie as of July 2025.
Sometimes people say that even if all AI&nbsp;&nbsp;

00:00:16.560 --> 00:00:20.880
progress totally stopped, the systems of&nbsp;
today would still be far more economically&nbsp;&nbsp;

00:00:20.880 --> 00:00:26.400
transformative than the internet. I disagree.&nbsp;
I think the LLMs of today are magical.&nbsp;

00:00:26.400 --> 00:00:31.920
But the reason that the Fortune 500 aren’t using&nbsp;
them to totally transform their workflows isn’t&nbsp;&nbsp;

00:00:31.920 --> 00:00:36.560
because the management there is too stodgy.
Rather, I think it’s genuinely hard to get&nbsp;&nbsp;

00:00:36.560 --> 00:00:41.440
normal humanlike labor out of these LLMs.
And this has to do with some fundamental&nbsp;&nbsp;

00:00:41.440 --> 00:00:46.400
capabilities these models lack.
I like to think I’m "AI forward"&nbsp;&nbsp;

00:00:46.400 --> 00:00:49.600
here at the Dwarkesh Podcast.
I’ve probably spent over a hundred&nbsp;&nbsp;

00:00:49.600 --> 00:00:53.200
hours trying to build these little LLM&nbsp;
tools for my post production setup.&nbsp;

00:00:53.200 --> 00:00:58.240
And the experience of trying to get them&nbsp;
to be useful has extended my timelines.&nbsp;

00:00:58.240 --> 00:01:03.280
I’ll try to get an LLMs to rewrite autogenerated&nbsp;
transcripts for me, to optimize for readability&nbsp;&nbsp;

00:01:03.280 --> 00:01:06.720
in the way a human would.
Or I’ll try to get them to&nbsp;&nbsp;

00:01:06.720 --> 00:01:12.240
identify clips from a transcript that I feed in.
Sometimes I’ll try to get them to co-write an&nbsp;&nbsp;

00:01:12.240 --> 00:01:18.080
essay with me, passage by passage.
These are simple, self contained,&nbsp;&nbsp;

00:01:18.080 --> 00:01:23.760
short horizon, language in, language out tasks—the&nbsp;
kinds of assignments that should be dead center&nbsp;&nbsp;

00:01:23.760 --> 00:01:30.560
in the LLMs’ repertoire. And they're 5/10 at&nbsp;
them. Don’t get me wrong, that is impressive.&nbsp;

00:01:30.560 --> 00:01:37.680
But the fundamental problem is that LLMs don’t&nbsp;
get better over time the way a human would.&nbsp;

00:01:37.680 --> 00:01:42.400
This lack of continual learning&nbsp;
is a huge huge bottleneck.&nbsp;

00:01:42.400 --> 00:01:47.120
The LLM baseline at many tasks might&nbsp;
be higher than the average human's.&nbsp;

00:01:47.120 --> 00:01:50.720
But there’s no way to give&nbsp;
a model high level feedback.&nbsp;

00:01:50.720 --> 00:01:53.840
You’re stuck with the abilities&nbsp;
you get out of the box.&nbsp;

00:01:53.840 --> 00:01:58.480
You can keep messing around with the system&nbsp;
prompt, but in practice this just doesn’t produce&nbsp;&nbsp;

00:01:58.480 --> 00:02:04.240
anything even close to the kind of learning and&nbsp;
improvement that human employees experience.&nbsp;

00:02:04.240 --> 00:02:08.320
The reason humans are so useful&nbsp;
is not mainly their raw intellect.&nbsp;

00:02:08.320 --> 00:02:12.640
It’s their ability to build up context,&nbsp;
interrogate their own failures,&nbsp;&nbsp;

00:02:12.640 --> 00:02:17.280
and pick up small improvements and&nbsp;
efficiencies as they practice a task.&nbsp;

00:02:17.280 --> 00:02:22.720
How would you teach a kid to play the saxophone?
You'd have them try to blow into one, and then&nbsp;&nbsp;

00:02:22.720 --> 00:02:28.000
they'd see how it sounds, and they'd adjust.
Now imagine if this was the way you'd have to&nbsp;&nbsp;

00:02:28.000 --> 00:02:30.640
teach saxophone instead: A&nbsp;
student takes one attempt.&nbsp;

00:02:30.640 --> 00:02:34.320
And the moment they make a mistake, you&nbsp;
send them away and you write detailed&nbsp;&nbsp;

00:02:34.320 --> 00:02:38.240
instructions about what went wrong.
And you call the next student in.&nbsp;

00:02:38.240 --> 00:02:42.080
And the next student reads your notes&nbsp;
and tries to play Charlie Parker cold.&nbsp;

00:02:42.080 --> 00:02:47.360
And when they fail, you refine your instructions&nbsp;
and you invite the next student. This just&nbsp;&nbsp;

00:02:47.360 --> 00:02:52.800
wouldn’t work. No matter how well honed your&nbsp;
prompt is, no kid is just going to learn how&nbsp;&nbsp;

00:02:52.800 --> 00:02:59.760
to play saxophone from reading your instructions.
But this is the only modality we have to ‘teach’&nbsp;&nbsp;

00:02:59.760 --> 00:03:07.440
LLMs anything. Yes, there’s RL fine tuning.&nbsp;
But it’s not a deliberate, adaptive process&nbsp;&nbsp;

00:03:07.440 --> 00:03:11.360
in the way that human learning is.
My editors have gotten extremely good.&nbsp;

00:03:11.360 --> 00:03:16.720
And they wouldn’t have gotten that way if&nbsp;
we had to build bespoke RL environments for&nbsp;&nbsp;

00:03:16.720 --> 00:03:21.040
different subtasks involved in their work.
They’ve just noticed a lot of small things&nbsp;&nbsp;

00:03:21.040 --> 00:03:25.120
themselves and thought hard about&nbsp;
what resonates with the audience,&nbsp;&nbsp;

00:03:25.120 --> 00:03:30.320
what kind of content I like, and how they&nbsp;
can improve their day to day workflows.&nbsp;

00:03:30.320 --> 00:03:34.080
Now, it’s possible to imagine ways in&nbsp;
which a smarter model could build a&nbsp;&nbsp;

00:03:34.080 --> 00:03:38.960
dedicated RL loop for itself which just&nbsp;
feels super organic from the outside.&nbsp;

00:03:38.960 --> 00:03:43.280
I give some high level feedback, and the&nbsp;
model comes up with a bunch of verifiable&nbsp;&nbsp;

00:03:43.280 --> 00:03:49.280
practice problems to RL on—maybe even a whole&nbsp;
environment in which it gets to rehearse the&nbsp;&nbsp;

00:03:49.280 --> 00:03:52.480
skills that it thinks it's lacking.
But this just sounds really hard.&nbsp;

00:03:52.480 --> 00:03:54.800
And I don’t know how well these&nbsp;
techniques will generalize to&nbsp;&nbsp;

00:03:54.800 --> 00:03:59.360
different kinds of tasks and feedback.
Eventually the models will be able to learn&nbsp;&nbsp;

00:03:59.360 --> 00:04:05.120
on the job in this organic way that humans can.
But it’s just hard for me to see how that could&nbsp;&nbsp;

00:04:05.120 --> 00:04:09.280
happen within the next few years, given&nbsp;
there’s no immediately obvious way in&nbsp;&nbsp;

00:04:09.280 --> 00:04:14.160
which to slot in continuous learning into&nbsp;
the kinds of models that these LLMs are.&nbsp;

00:04:14.160 --> 00:04:17.120
LLMs actually do get kinda smart and&nbsp;
useful in the middle of a session.&nbsp;

00:04:17.120 --> 00:04:20.160
For example, sometimes I’ll&nbsp;
co-write an essay with an LLM.&nbsp;

00:04:20.160 --> 00:04:24.560
I’ll give it an outline, and I’ll ask it&nbsp;
to draft the essay passage by passage.&nbsp;

00:04:24.560 --> 00:04:28.480
And all its suggestions up till&nbsp;
paragraph four will just be bad.&nbsp;

00:04:28.480 --> 00:04:34.080
I'll just rewrite every single paragraph from&nbsp;
scratch and tell it, "Look, your shit sucked.&nbsp;

00:04:34.080 --> 00:04:38.080
This is what I wrote instead."
And at this point, it will actually start&nbsp;&nbsp;

00:04:38.080 --> 00:04:42.960
giving good suggestions for the next paragraph.
But this whole subtle understanding of my&nbsp;&nbsp;

00:04:42.960 --> 00:04:46.720
preferences and style will just&nbsp;
be lost by the end of the session.&nbsp;

00:04:47.360 --> 00:04:52.240
Maybe there is an easy solution to this that&nbsp;
looks like a long rolling context window, like&nbsp;&nbsp;

00:04:52.240 --> 00:04:58.000
Claude Code already has, which just compacts the&nbsp;
session memory into a summary every 30 minutes.&nbsp;

00:04:58.000 --> 00:05:03.440
I just think that titrating all this rich&nbsp;
tacit experience into a text summary will&nbsp;&nbsp;

00:05:03.440 --> 00:05:07.360
be brittle in domains outside of software&nbsp;
engineering, which is very text-based,&nbsp;&nbsp;

00:05:07.360 --> 00:05:12.400
in which you already have this external scaffold&nbsp;
of memory that is stored in the codebase itself.&nbsp;

00:05:12.400 --> 00:05:19.120
Again, think about what it would be like to&nbsp;
teach a kid to play the saxophone just from text.&nbsp;

00:05:19.120 --> 00:05:25.760
Even Claude Code will often reverse a hard-earned&nbsp;
optimization that we engineered together before I&nbsp;&nbsp;

00:05:25.760 --> 00:05:31.760
hit /compact —because the explanation for why&nbsp;
it was made didn’t make it into the summary.&nbsp;

00:05:31.760 --> 00:05:36.000
This is why I disagree with something that&nbsp;
Anthropic researchers Sholto Douglas and&nbsp;&nbsp;

00:05:36.000 --> 00:05:40.320
Trenton Bricken said on my podcast.
And this quote is from Trenton:&nbsp;

00:05:40.320 --> 00:05:43.840
"Even if AI progress totally stalls&nbsp;
(and you think that the models are&nbsp;&nbsp;

00:05:43.840 --> 00:05:48.800
really spiky, and they don't have general&nbsp;
intelligence), it's so economically valuable,&nbsp;&nbsp;

00:05:48.800 --> 00:05:55.520
and sufficiently easy to collect data on all&nbsp;
of these different white collar job tasks,&nbsp;&nbsp;

00:05:55.520 --> 00:06:00.640
such that to Sholto's point we should expect to&nbsp;
see them automated within the next five years."&nbsp;

00:06:00.640 --> 00:06:06.400
If AI progress totally stops today, I think less&nbsp;
than 25% of white collar employment goes away.&nbsp;

00:06:06.400 --> 00:06:12.720
Sure, many tasks will get automated.
Claude 4 Opus can technically rewrite&nbsp;&nbsp;

00:06:12.720 --> 00:06:17.120
autogenerated transcripts for me.
But since it’s not possible for me&nbsp;&nbsp;

00:06:17.120 --> 00:06:22.640
to have it improve over time and learn my&nbsp;
preferences, I still hire a human for this.&nbsp;

00:06:22.640 --> 00:06:28.480
So even if we get more data, without progress&nbsp;
in continual learning, I think that we will be&nbsp;&nbsp;

00:06:28.480 --> 00:06:33.120
in a substantially similar position with&nbsp;
all other kinds of white collar work.&nbsp;

00:06:33.120 --> 00:06:38.800
Yes, technically AIs will be able to do&nbsp;
a lot of subtasks somewhat satisfactorily,&nbsp;&nbsp;

00:06:38.800 --> 00:06:44.160
but their inability to build up context&nbsp;
will make it impossible to have them operate&nbsp;&nbsp;

00:06:44.160 --> 00:06:48.240
as actual employees at your firm.
While this makes me bearish about&nbsp;&nbsp;

00:06:48.240 --> 00:06:55.280
transformative AI in the next few years, it makes&nbsp;
me especially bullish on AI over the next decades.&nbsp;

00:06:55.280 --> 00:07:01.120
When we do solve continual learning, we’ll see a&nbsp;
huge discontinuity in the value of these models.&nbsp;

00:07:01.120 --> 00:07:05.360
Even if there isn’t a software only&nbsp;
singularity, where these models rapidly&nbsp;&nbsp;

00:07:05.360 --> 00:07:10.800
build smarter and smarter successor systems,&nbsp;
we might still get something that looks like&nbsp;&nbsp;

00:07:10.800 --> 00:07:14.560
a broadly deployed intelligence explosion.
AIs will be getting broadly deployed through the&nbsp;&nbsp;

00:07:14.560 --> 00:07:20.080
economy, and doing different jobs and learning&nbsp;
while doing them in the way that humans can.&nbsp;

00:07:20.080 --> 00:07:24.240
However, unlike humans, these&nbsp;
models can amalgamate their&nbsp;&nbsp;

00:07:24.240 --> 00:07:29.040
learnings across all their copies.
So one AI is basically learning&nbsp;&nbsp;

00:07:29.040 --> 00:07:34.320
how to do every single job in the economy.
An AI that is capable of this kind of online&nbsp;&nbsp;

00:07:34.320 --> 00:07:42.160
learning might rapidly become a superintelligence&nbsp;
even if there's no further algorithmic progress.&nbsp;

00:07:42.160 --> 00:07:45.840
However, I’m not expecting to watch some&nbsp;
OpenAI livestream where they announce that&nbsp;&nbsp;

00:07:45.840 --> 00:07:50.320
continual learning has been totally solved.
Because labs are incentivized to release any&nbsp;&nbsp;

00:07:50.320 --> 00:07:55.280
innovations quickly, we’ll see a broken early&nbsp;
version of continual learning (or test time&nbsp;&nbsp;

00:07:55.280 --> 00:08:01.280
training, or whatever you want to call it) before&nbsp;
we see something which truly learns like a human.&nbsp;

00:08:01.280 --> 00:08:08.320
I expect to get lots of heads up before&nbsp;
this big bottleneck is totally solved.&nbsp;

00:08:08.320 --> 00:08:12.240
When I interviewed Anthropic researchers&nbsp;
Sholto Douglas and Trenton Bricken on my&nbsp;&nbsp;

00:08:12.240 --> 00:08:17.920
podcast, they said that they expect reliable&nbsp;
computer use agents by the end of next year.&nbsp;

00:08:17.920 --> 00:08:23.200
We already have computer use agents right&nbsp;
now, but they’re pretty bad. They’re imagining&nbsp;&nbsp;

00:08:23.200 --> 00:08:28.000
something quite different. Their forecast is&nbsp;
that by the end of next year, you should be&nbsp;&nbsp;

00:08:28.000 --> 00:08:32.400
able to tell an AI, "Go do my taxes."
And it'ill go through all your email,&nbsp;&nbsp;

00:08:32.400 --> 00:08:37.200
your Amazon orders, and Slack messages, and&nbsp;
it will email back and forth with every single&nbsp;&nbsp;

00:08:37.200 --> 00:08:42.720
person you need to get invoices from, it'll&nbsp;
compile all your receipts, decide what things&nbsp;&nbsp;

00:08:42.720 --> 00:08:46.960
are actually are business expenses, and it will&nbsp;
ask for your approval on all the edge cases,&nbsp;&nbsp;

00:08:46.960 --> 00:08:53.600
and then will just submit Form 1040 to the IRS.&nbsp;
I’m skeptical. I’m not an AI researcher, so far be&nbsp;&nbsp;

00:08:53.600 --> 00:08:58.640
it for me to contradict them on technical details.
But given what little I know, here’s why I’d bet&nbsp;&nbsp;

00:08:58.640 --> 00:09:01.040
against this forecast:
One.&nbsp;

00:09:01.040 --> 00:09:04.960
As horizon lengths increase,&nbsp;
rollouts have to become longer.&nbsp;

00:09:04.960 --> 00:09:09.120
The AI needs to do two hours worth of&nbsp;
agentic computer use tasks before we&nbsp;&nbsp;

00:09:09.120 --> 00:09:12.640
can even see if it did it right.
Not to mention that computer use&nbsp;&nbsp;

00:09:12.640 --> 00:09:16.960
requires processing images and video,&nbsp;
which is already more compute intensive,&nbsp;&nbsp;

00:09:16.960 --> 00:09:22.880
even if you don’t factor in the longer rollouts.
This seems like it should slow down progress.&nbsp;&nbsp;

00:09:22.880 --> 00:09:28.960
Two. We don’t have a large pretraining&nbsp;
corpus of multimodal computer use data.&nbsp;

00:09:28.960 --> 00:09:32.800
I like this quote from Mechanize’s post&nbsp;
on automating software engineering:&nbsp;&nbsp;

00:09:33.520 --> 00:09:37.840
"For the past decade of scaling, we’ve been&nbsp;
spoiled by the enormous amount of internet&nbsp;&nbsp;

00:09:37.840 --> 00:09:42.720
data that was freely available for us to use.
This was enough to crack natural language&nbsp;&nbsp;

00:09:42.720 --> 00:09:47.840
processing, but not for getting models&nbsp;
to become reliable, competent agents.&nbsp;

00:09:47.840 --> 00:09:53.520
Imagine trying to train GPT-4 on all&nbsp;
the text data available in 1980—the&nbsp;&nbsp;

00:09:53.520 --> 00:09:57.840
data would have been nowhere near enough,&nbsp;
even if you had the necessary compute."&nbsp;

00:09:58.560 --> 00:10:03.360
Again, I’m not at the labs.
Maybe text only training already gives you a&nbsp;&nbsp;

00:10:03.360 --> 00:10:08.800
great prior over how different UIs work, and what&nbsp;
the relationship is between different components.&nbsp;

00:10:08.800 --> 00:10:13.760
Maybe RL fine tuning is so sample efficient&nbsp;
that you don’t need that much data.&nbsp;

00:10:13.760 --> 00:10:18.880
But I haven’t seen any public evidence which&nbsp;
makes me think that these models have suddenly&nbsp;&nbsp;

00:10:18.880 --> 00:10:24.320
gotten less data hungry, especially in domains&nbsp;
where they’re substantially less practiced.&nbsp;

00:10:24.320 --> 00:10:29.200
Alternatively, maybe these models are such&nbsp;
good front end coders that they can just&nbsp;&nbsp;

00:10:29.200 --> 00:10:36.240
generate millions of toy UIs for themselves&nbsp;
to practice on. But, three. Even algorithmic&nbsp;&nbsp;

00:10:36.240 --> 00:10:42.080
innovations which seem quite simple in&nbsp;
retrospect took a long time to iron out.&nbsp;

00:10:42.080 --> 00:10:48.320
The RL procedure which DeepSeek explained in&nbsp;
their R1 paper seems simple at a high level.&nbsp;

00:10:48.320 --> 00:10:54.720
And yet it took 2 years from the development&nbsp;
and launch of GPT-4 to the release of o1.&nbsp;

00:10:54.720 --> 00:11:00.800
Now of course I know that it's insanely and&nbsp;
hilariously arrogant to say that R1/o1 were easy—a&nbsp;&nbsp;

00:11:00.800 --> 00:11:06.240
ton of engineering, debugging, and pruning of&nbsp;
alternative ideas was required to arrive at&nbsp;&nbsp;

00:11:06.240 --> 00:11:11.680
this solution. But that’s precisely my point!&nbsp;
Seeing how long it took to implement the idea&nbsp;&nbsp;

00:11:11.680 --> 00:11:17.680
of ‘We should train a model to solve verifiable&nbsp;
math and coding problems,’ makes me think that&nbsp;&nbsp;

00:11:17.680 --> 00:11:22.480
we’re underestimating the difficulty of solving&nbsp;
the much gnarlier problem of computer use, where&nbsp;&nbsp;

00:11:22.480 --> 00:11:30.720
you’re operating in a totally different modality&nbsp;
with much less data. Okay, enough cold water. I’m&nbsp;&nbsp;

00:11:30.720 --> 00:11:35.360
not going to be like one of those spoiled children&nbsp;
on Hackernews who could be handed a golden-egg&nbsp;&nbsp;

00:11:35.360 --> 00:11:39.760
laying goose and would still spend all their&nbsp;
time complaining about how loud its quacks are.&nbsp;

00:11:39.760 --> 00:11:46.480
Have you read the reasoning traces from o3 or&nbsp;
Gemini 2.5? It’s actually reasoning! It’s breaking&nbsp;&nbsp;

00:11:46.480 --> 00:11:52.800
down a problem, thinking through what the user&nbsp;
wants, reacting to its own internal monologue,&nbsp;&nbsp;

00:11:52.800 --> 00:11:57.440
and correcting itself when it notices that&nbsp;
it's pursuing an unproductive direction.&nbsp;

00:11:57.440 --> 00:12:02.000
How are we just like, "Oh yeah of course a&nbsp;
machine is gonna go think a bunch, come up&nbsp;&nbsp;

00:12:02.000 --> 00:12:06.480
with a bunch of ideas, and come back to me with&nbsp;
a smart answer. That’s just what machines do."&nbsp;

00:12:06.480 --> 00:12:10.000
Part of the reason some people are too&nbsp;
pessimistic is that they haven’t played&nbsp;&nbsp;

00:12:10.000 --> 00:12:14.880
around with the smartest models in&nbsp;
domains where they’re the most competent.&nbsp;

00:12:14.880 --> 00:12:19.760
Giving Claude Code a vague spec and just&nbsp;
sitting around for 10 minutes while it&nbsp;&nbsp;

00:12:19.760 --> 00:12:25.920
zero shots a working application is a wild&nbsp;
experience. How did it do that? You can talk&nbsp;&nbsp;

00:12:25.920 --> 00:12:31.760
about circuits and the training distribution&nbsp;
and RL or whatever, but the most proximal,&nbsp;&nbsp;

00:12:31.760 --> 00:12:38.160
concise, and accurate explanation is simply that&nbsp;
it’s powered by a baby general intelligence.&nbsp;

00:12:38.160 --> 00:12:42.960
At this point, part of you has to&nbsp;
be thinking, "It’s actually working.&nbsp;

00:12:42.960 --> 00:12:50.400
We’re making machines that are intelligent."
My probability distributions are super wide.&nbsp;

00:12:50.400 --> 00:12:53.920
And I want to emphasize that I do&nbsp;
believe in probability distributions.&nbsp;

00:12:53.920 --> 00:12:59.920
Which means that work to prepare for a&nbsp;
misaligned 2028 ASI still makes a ton of sense.&nbsp;

00:12:59.920 --> 00:13:03.840
I think this is a totally plausible outcome.
But here are the timelines at which&nbsp;&nbsp;

00:13:03.840 --> 00:13:07.280
I’d take a 50/50 bet.
An AI that can do taxes&nbsp;&nbsp;

00:13:07.280 --> 00:13:12.160
end-to-end for my small business as well as&nbsp;
a competent general manager could in a week:&nbsp;&nbsp;

00:13:12.160 --> 00:13:17.920
including chasing down all the receipts on&nbsp;
different websites, finding the missing pieces,&nbsp;&nbsp;

00:13:17.920 --> 00:13:22.240
emailing back and forth with anyone who we need&nbsp;
to hassle for invoices, filling out the form,&nbsp;&nbsp;

00:13:22.240 --> 00:13:28.800
and sending it to the IRS. This I'd say 2028. I&nbsp;
think we’re in the GPT 2 era for computer use.&nbsp;

00:13:28.800 --> 00:13:33.600
But we have no pretraining corpus, and the&nbsp;
models are optimizing for a much sparser&nbsp;&nbsp;

00:13:33.600 --> 00:13:39.120
reward over a much longer time horizon using&nbsp;
action primitives they’re unfamiliar with.&nbsp;

00:13:39.120 --> 00:13:42.400
That being said, the base model is&nbsp;
already decently smart and might have&nbsp;&nbsp;

00:13:42.400 --> 00:13:47.600
a good prior over computer use tasks,&nbsp;
plus there’s a lot more compute and AI&nbsp;&nbsp;

00:13:47.600 --> 00:13:53.040
researchers in the world, so it might even out.
Preparing taxes for a small business feels like&nbsp;&nbsp;

00:13:53.040 --> 00:14:00.880
for computer use what GPT 4 was for language.
It took 4 years to get from GPT 2 to GPT 4.&nbsp;

00:14:00.880 --> 00:14:04.480
Just to clarify, I am not saying that&nbsp;
we won’t have really cool computer use&nbsp;&nbsp;

00:14:04.480 --> 00:14:08.640
demos in 2026 and 2027.
GPT-3 was super cool,&nbsp;&nbsp;

00:14:08.640 --> 00:14:13.360
but it was not that practically useful.
I’m saying that these models won’t be capable&nbsp;&nbsp;

00:14:13.360 --> 00:14:19.040
of end-to-end handling a week long and quite&nbsp;
involved project which involves computer use.&nbsp;

00:14:19.040 --> 00:14:24.160
Ok, and as for the forecast of when AI&nbsp;
will be able to learn on the job as easily,&nbsp;&nbsp;

00:14:24.160 --> 00:14:29.280
organically, seamlessly, and quickly&nbsp;
as humans, for any white collar work.&nbsp;

00:14:29.280 --> 00:14:35.040
For example, if I hired an AI video editor, after&nbsp;
six months it would have as much actionable,&nbsp;&nbsp;

00:14:35.040 --> 00:14:39.520
deep understanding of my preferences,&nbsp;
our channel, what works for the audience,&nbsp;&nbsp;

00:14:39.520 --> 00:14:44.000
as well as a human would.
I'd say this would come in 2032.&nbsp;

00:14:44.000 --> 00:14:47.840
While I don’t see an obvious way to&nbsp;
slot in continuous online learning&nbsp;&nbsp;

00:14:47.840 --> 00:14:53.280
into the kinds of models these LLMs&nbsp;
are, 7 years is a really long time!&nbsp;

00:14:53.280 --> 00:14:58.320
GPT 1 had just come out this time 7 years ago.
It doesn’t seem implausible to me that over the&nbsp;&nbsp;

00:14:58.320 --> 00:15:04.080
next 7 years, we’ll find some way to get&nbsp;
these models to actually learn on the job.&nbsp;

00:15:04.080 --> 00:15:06.960
At this point you might be reacting,&nbsp;
"Wait you made this huge fuss about&nbsp;&nbsp;

00:15:06.960 --> 00:15:11.520
continual learning being such a huge handicap.
But then your prediction is that we’re 7 years&nbsp;&nbsp;

00:15:11.520 --> 00:15:17.680
away from what, at a minimum, looks like a broadly&nbsp;
deployed intelligence explosion." And yeah, you’re&nbsp;&nbsp;

00:15:17.680 --> 00:15:24.160
right. I am forecasting a pretty wild world within&nbsp;
a relatively short amount of time. AGI timelines&nbsp;&nbsp;

00:15:24.160 --> 00:15:30.240
are very lognormal. It's either this decade or&nbsp;
bust. (Not really, it's more like lower marginal&nbsp;&nbsp;

00:15:30.240 --> 00:15:35.040
probability per year—but that’s less catchy).
AI progress over the last decade has&nbsp;&nbsp;

00:15:35.040 --> 00:15:38.640
been driven by scaling training&nbsp;
compute on the frontier systems.&nbsp;

00:15:38.640 --> 00:15:43.040
It's been over 4x a year.
This cannot continue beyond this decade,&nbsp;&nbsp;

00:15:43.040 --> 00:15:48.720
whether you look at chips, power, or even the&nbsp;
raw fraction of GDP that is used on training.&nbsp;

00:15:48.720 --> 00:15:53.520
After 2030, AI progress has to mostly&nbsp;
come from algorithmic progress.&nbsp;

00:15:53.520 --> 00:15:56.560
But even there all the low&nbsp;
hanging fruit will be plucked,&nbsp;&nbsp;

00:15:56.560 --> 00:16:02.560
at least under the deep learning paradigm.
So the yearly probability of AGI collapses.&nbsp;

00:16:02.560 --> 00:16:08.320
This means that if we end up on the longer side of&nbsp;
my 50/50 bets, we might be looking at a relatively&nbsp;&nbsp;

00:16:08.320 --> 00:16:15.120
normal world up till the 2030s or even the 2040s.
But in all the other worlds, even if we stay sober&nbsp;&nbsp;

00:16:15.120 --> 00:16:20.960
about the current limitations of AI, we&nbsp;
have to expect some truly crazy outcomes.&nbsp;

00:16:21.920 --> 00:16:27.280
This was originally a blog post that I&nbsp;
published on my website at dwarkesh.com.&nbsp;

00:16:27.280 --> 00:16:32.160
And it was obviously inspired by the discussion&nbsp;
I had with Sholto and Trenton on my podcast,&nbsp;&nbsp;

00:16:32.160 --> 00:16:36.880
where I ended up disagreeing with them about&nbsp;
timelines but it took me a few weeks of thinking&nbsp;&nbsp;

00:16:36.880 --> 00:16:41.440
afterwards, sorting out exactly where I&nbsp;
disagree and why I had longer timelines.&nbsp;

00:16:41.440 --> 00:16:46.560
And I do this for other episodes as well.
I wrote up some thoughts I had about the many&nbsp;&nbsp;

00:16:46.560 --> 00:16:51.760
thousands of pages that Stephen Kotkin has written&nbsp;
about Stalin, obviously which we were not able to&nbsp;&nbsp;

00:16:51.760 --> 00:16:56.880
exhaustively cover in that one 2-hour interview.
So anyways, if you want to see these additional&nbsp;&nbsp;

00:16:56.880 --> 00:17:02.240
artifacts and writing that I produce as a result&nbsp;
of this podcast and in preparation for episodes,&nbsp;&nbsp;

00:17:02.240 --> 00:17:07.840
you should subscribe to my blog and newsletter.
You can do that at dwarkesh.com.&nbsp;

00:17:07.840 --> 00:17:12.720
Otherwise I will also see you next week&nbsp;
for a full episode with a real guest.

