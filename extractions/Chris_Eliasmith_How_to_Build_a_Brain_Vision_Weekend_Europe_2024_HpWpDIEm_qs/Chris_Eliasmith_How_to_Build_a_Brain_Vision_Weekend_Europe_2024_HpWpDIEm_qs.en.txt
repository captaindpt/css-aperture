**Transcript extracted from YouTube video**

Kind: captions
Language: en
uh I'm going to do something I think a
little bit different than what you've
seen so far so I'm going to take you
through work that we've done in building
a large scale brain model and then
hopefully getting to my last few slides
think about what that means for the
future and how this relates to other
Technologies so the model that we built
is called spawn uh it's the world's
largest functional brain model or it was
in 2012 when we first proposed it uh it
showed up in two places one was chapter
seven of this book I wrote called how to
build a brain that came out with Oxford
and the other place was in science uh it
got of press which was great and at the
time uh you know it had about 2 and half
million neurons 7.5 billion connections
which was a lot in 2012 right bigger
than most neural networks uh and it did
a lot of sort of modeling of
Neuroscience so it had the right variety
of neurotransmitters we tried to map
match both the physiology and the
anatomy and one of the key things is
that it was functional right so it was a
brain model that actually did something
uh which you know is not all that common
surprisingly might
think uh at the time we had eight
different tasks that we were looking at
uh and what I'm going to talk today
about today is mostly what we did six
years after that which we call spawn 2.0
uh so here's an example of the model
running it's performing a task where
it's comparing the categories that
images are coming from so there it saw
two monkeys so it wrote a one saying
yeah that was the same category so it's
looking at these different images and
then you know writing down what it sees
uh this is a experiment that you can run
on humans and exactly the same way uh
now it's just writing out the digit
class of Digit that it sees and it's got
the same input and output so the input
is something projected on screen the
output is movement of a physically
modeled arm so it's actually generating
torqux and muscles in order to move the
arm and so on but of course unlike a
human we can look at every detail that
we want about this Behavior how this
behavior is generated uh so here's an
example of what's going on in the
primary visual cortex uh and there you
can see a bunch of spikes that are being
generated in the model and then on top
we're saying well how would we decode
those spikes that's not enough to
actually perform this task you have to
remember what you saw um and so we also
have areas which are doing things like
working memory uh and of course you have
to change what you remember you saw into
a motor plan which will then actually
let you drive your uh arm through the
pattern that replicates what you saw and
so we also have a motor system uh but of
course this is a fairly low-level task
right this is something that from a
machine learning perspective is fairly
simple we're kind of doing mnist at this
point um but we can with the same exact
model do much more complicated tasks so
here's a cognitive task um so you know
your job is to figure out what goes in
that box at the end and we've built up
you know this same model is doing the
kind of reasoning that you're doing I'll
presume you've never seen this
particular task before this is patterned
after an intelligence test that tests
fluid intelligence called Ravens
Progressive matrices and you're looking
across all this data and trying to infer
a pattern and figuring out how you'd
complete that pattern uh so when our
model does this it's doing the same kind
of thing right it's never seen this
before it hasn't memorized this um
particular uh set of inputs it can't
move its eye around so we're showing the
data in a slightly different format but
it's the same uh sort of structure and
it's sitting there and basically doing
inference to the best explanation right
so once I get at the end what would I do
in order to complete the pattern and so
like you it will write out three fives
right saying that that would be the
correct way to complete this
pattern so what we like about that is
this shows that you know the same
techniques and model can do low-level
stuff but it can also do cognitive stuff
uh and we also like to point out this is
a brain model so you know we can take
data uh here it's doing a task that I
haven't shown you which is a bandit task
and we can compare the Dynamics of the
spikes generated by the model to the
Dynamics of spikes recorded from a
rodent in the same part of the brain
right in the ventral stum both the model
and the real one and then we can look at
the Dynamics and see are they the same
or different and and go from there but
we can also do this at a behavioral
level so we can show a whole bunch of uh
images uh different numbers different
numbers of numbers that people or and
the model will memorize and repeat back
and you can look at patterns in working
memory you can see what are known as
Primacy and recency effects in
Psychology where you have really good
memory of the beginning at the end of
the list and sort of the quality of your
memory gets worse in the middle and so
on uh and we can go uh to fairly low
levels of detail so in this model uh the
model in the front we call spawn plus
because what we've done is we've taken a
lot of the very simple neurons in spawn
and we've replaced them with very
complicated neuron models so we can
actually look at the effects of uh
changing ions um in this case we're
applying a drug called TTX which is a
sodium channel blocker to the model in
the front and we can see how does it
perform this working memory task and you
can see it remembers the information and
then it starts writing out its answer
but it stops and the reason it stops is
shown by the red box where it's kind of
all fuzzy in there and it means it it
hasn't forgotten so we can see the 432
but it doesn't remember what it was
doing right so part of the model is
keeping track of what task it's in and
it's really that's the thing that you
disrupted when you injected this sodium
channel blocker so what I like about
this example you'd never ethically do
this experiment to a human right but now
we have a really clear mechanistic
hypothesis about what the relationship
between this low-level you know
molecular intervention and a high level
cognitive effect uh could plausibly
be so the other thing that we're
actually I guess one thing I should
mention about spawn 2.0 is it has uh
about 6 million neurons and 20 billion
connections uh and so when it came out
you know it was really at the scale of
large language models at the time um and
what we'd like to do of course is to
keep growing the size of the model but
now we're running into computational
constraints and so on but spawn 3.0
basically Now's the Time right we've we
waited the right length of time one of
the other thing that we do with these
spawn models is looking at how do you
coordinate all of the control in the
brain to be able to do many different
tasks in the same substrate so the exact
same model with no changes to it can do
all of these tasks you just tell it
which task uh scenario it's in and then
it will perform the task given the right
inputs so you know in spawn 2.0 we now
have 12 tasks as opposed to eight um and
you'll notice that the very last one
there is instruction following and of
course that's a very general task that
can do uh for which you can do many
different things and I sometimes like to
call this mental gymnastics so I'm going
to give you a little example of what I
mean so imagine the letter V capital V
and then imagine a capital B rotate it
90Â° counterclockwise put it on top of
the V and then erase the back of the B
what shape do you
have heart see so some people followed
along I threw it at you pretty quick
yeah heart or a double scooped ice cream
cone is the other one you often get so
what you were doing there is basically
you know imagining things you're Jing
representations in your head you were
manipulating those and so on so in the
instruction following right we're doing
the same kind of thing with the spawn
model here uh so this is also pretty
hard to follow but at the beginning
we're saying okay find a pattern and
we're going to show you some digits this
is just like that Ravens Matrix so if
you know three follows one four follows
two what was the pattern is that it's
increasing by two now do question
answering right here is three digits
what was in the second position and
wrote out a seven now we're going to
apply the pattern that we found to that
list which means add the pattern to the
list so it's writing out 6 9 and four so
it's added two to the pattern you showed
it and then we're saying okay apply the
pattern to the position that you found
what do you get 2 plus 2 is four right
so it's you know doing the same kind of
thing that you were doing at that point
in time all right so uh now I get to the
slides I wanted to get to which is
thinking about the relationship of this
to other Technologies so you know what
we're doing here is integrating
neuroscience science and cognitive
science with uh using computational
Theory so this has a lot of implications
for neuromorphics and in fact our
techniques have been used to program
pretty much every neuromorphic chip
that's available right now uh and
there's you know trying to scale
neuromorphics to the point where it can
run a full spawn model I think is a huge
challenge so right now none of those
systems are very capable of doing that
looking at things like neural
Prosthetics of course is relevant
because we have an understanding of the
neural code how information is encoded
and transmitted um and it you know our
techniques have also been used to do
things like uh neuro prosthetics so far
just in
monkeys um thinking about AI as a
process which is not what llms are right
so this is thinking about them changing
over time interacting with constantly
changing environments and so on uh this
is you know very different than the sort
of standard techniques people tend to
use in AI uh less so with reinforcement
learning but definitely things like llms
but that's exactly what we've done here
and this is exactly how biological
cognition functions uh and then last
discovering new algorithms so I had this
great uh conversation over lunch about a
new algorithm that we've discovered
using these techniques which outperforms
Transformers by a fairly sign
significant margin and I think there's a
lot more to be discovered than that and
I'm happy to talk to people more about
that bold claim later on uh and then the
future so I think over the next five
years a lot more of this Ai and brain
model cross fertilization should happen
uh so we see it have you know it's
happened quite efficiently and
effectively in Vision research um and
it's just starting now in language
research so understanding the brain's
role in language as well you know using
some of the techniques of large language
models
a lot more of that can happen um
beginning to see really the first broad
commercial applications of some of this
kind of Technology then 10 years uh out
you know looking at things like can we
embody these large Brain models right so
we get up to you know trillions of
synapses uh billions of neurons and
being able to put those in robots which
are running on uh you know small amounts
of power ideally so uh I think doing
that is kind of taking the technical
route and then the other thing we want
to do is take the more Health oriented
route where we would want do something
like digital twinning right so where you
can scan a person uh or an animal and be
able to tune all of the parameters in
the model such that now when we run the
model it's actually acting like that
particular individual and I will stop
there thanks thank you