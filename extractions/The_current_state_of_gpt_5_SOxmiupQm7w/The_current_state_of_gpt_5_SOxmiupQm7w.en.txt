**Transcript extracted from YouTube video**

Kind: captions
Language: en
I'll be real with you guys. I don't even
know how to start this video. I don't
even know if I should be talking about
GBT5 still, but I'm getting a ton of
questions. I've spent the last couple
weeks trying to figure out what went
wrong and what we can do to fix it and
just generally trying to get the
experience I had for those first few
weeks when I had early access to GPT5.
The model is incredible and it is still
the model that I use for my work every
day. But that doesn't mean it's perfect.
That doesn't mean everything else is
irrelevant and it certainly doesn't mean
it's as good as it was when I was trying
it before. But I think the reasons why
are a lot more nuanced than people are
giving credit. Did I have a special
checkpoint that's no longer available?
Was I getting access to custom tools or
a special boosted version of chat GPT?
Were they secretly giving me GPT5 Pro
the whole time? There's a bunch of
questions and speculation and I've done
my best to try and figure out the truth
about all of this and also generally
analyze everything that went wrong. I
also found this awesome blog post from
Less Wrong, the reverse DeepSeek moment.
And I have so much here I want to share
and talk about from what went wrong and
why the perception was so bad to where
is this magically good model hiding
somewhere online. There's a lot to dive
into here and I want to do my best to
try and be transparent about what I
know, what I found, and everything I've
been told by every party involved. This
has been a lot more chaotic than I ever
could have imagined, but I do think the
actual problems are quite a bit simpler
than we might be giving them credit. All
of that said, I'm no longer getting free
access to any of this stuff and I have
spent honestly far too much money
testing it all. And since I'm not the
one editing this video, thank you FaZe
for taking over again. We got bills to
pay. Quick word from them and then we'll
get started. There have been a lot of
new models recently and they all claim
to be really good at tool calling in
code, but it's kind of hard to verify
that, much less get it running in your
real code bases unless you're using
today's sponsor, Kilo Code. These guys
are killing it. They built an open-
source all-in-one agent for VS Code. has
everything you need from orchestration
and architecture help to actually
writing code of course but also
debugging which is a surprisingly useful
feature I've found myself playing with a
lot more importantly though they support
every model and I mean it I use them for
testing all the weird drops that were
happening on open router but you can use
it for so much more if you've been
having problems using GPT5 which I know
a lot of us have been it works really
well in Kilo they don't charge any
premiums on the tokens that you use so
you're paying the cost you would
directly through the API obviously you
can bring your own API keys if you want
as well. But that means they can be
super transparent in the UI of how much
context is being used for each of your
requests. How nice is it that it tells
you what each request actually costs to
process, too, though. It's really good
for hopping between different models and
seeing what does and doesn't work
because you just get all of the
information. It doesn't hide these
things from you. It really does feel
like these guys want you to have full
clarity into what the coding agents are
doing. Right now, I have a generating my
image studio demo using a vanilla
next.js setup. Should be a pretty good
example. When you give GBD5 the right
tools to write code, the results are
stunning. And Kilo is clearly providing
those correct tools. I am blown away
with how this came out. And if you want
to see the magic yourself, check them
out today at soovive.link/kilo. By the
way, they gave me a code that gets you
another $13.37 of credit. Make sure that
you use that. That's a lot of inference
for free. I'm very excited to dive into
this article with you guys, but I want
to start somewhere slightly different.
What went wrong? Let's actually start
with all of the questions that I keep
getting asked so we have them all in one
place. that I have a special version is
the most common one. Was this version
boosted in some way? Is the public
version handicapped? Was I actually
using GBT5 Pro? Is the snapshot of the
model I use gone forever? There's a lot
of flavors of this question. The core
thing being people seem to believe that
what I was using during the testing is
fundamentally different from what you
are using as a user. I have come to the
conclusion it is not that simple. To put
it very lightly, for full transparency,
I want to show you guys what I was using
when I was using the model early. When I
was testing, the majority of my testing
was done on this particular snapshot,
GBT5 reasoning alpha new reasoning
effort 2025725.
So, a lot of my testing was on this
snapshot, but there was a slightly
different version that had CIS prompt
put in the slug. The reason for that is
that the CIS prompt piece indicated that
this was the special version with a
custom system prompt that OpenAI wrote
specifically for cursor because there
was a lot of collab in a lot of weird
ways going on between cursor and OpenAI
because on one hand they had to try and
make it so the model would work well in
cursor but on the other hand they had to
try and make it not clear that GBT 5 was
available in quotes in cursor and even
if you tried using it you wouldn't be
able to you had to hit the API directly
through OpenAI. So I had to bring my own
key for all of this. When I turn on the
open AI API key, then I could access the
GBD5 models that we had access to. The
reasoning alpha new reasoning version
was the general GPD5 we recommended to
use over the API. But if we were working
in cursor specifically, we were told to
use this image instead. This snapshot is
the one that I was using in cursor. To
be clear, these are both the exact same
model. The difference was that this API
endpoint had a system prompt inserted
into it that forced it to behave a
specific way. It is also of note that
the GPD5 endpoint did not accept a lot
of parameters like temperature. So you
couldn't really configure how it behaved
with a bunch of those random magic
variables. I already see people
speculating about that. The GBD5
reasoning alpha cy prompt version is
what I did the majority of my coding in
when I was using cursor with GBT5 before
the launch and that was what I had the
crazy experience that resulted in that
first video with. So that solves it,
right? This magic version of the model
that I was using is really good and
smart and capable and the version
everybody else is using is dumb and
wrong in a different snapshot that's
gimped and intentionally handicapped
because they don't want to give the best
to everyone. Obviously, right? Not that
simple. The reason that this special
slug existed was because of all the back
and forth going on between cursor and
OpenAI in order to implement things.
When you have something like this where
the two parts are so separate from each
other where OpenAI has a bunch of layers
of obuscation for their new model that
they haven't put out officially yet and
cursor has to implement features to make
that model work well without showing
that they have this and that this thing
exists. That resulted in a lot of
friction that resulted in a lot of
different parts being shoved in
different places. Some parts were being
shoved into the GPT5 endpoints to fix
the system prompt. Other parts were
being shoved into the editor with custom
prefixes. There was a fun moment where I
was so scared of leaking I really wanted
to have access to a different version of
this which was it was called nectarine
alpha new reasoning effort0725.
This and this were the exact same model
the exact same endpoint just different
strings that point to the same thing. I
wanted to be able to use the nectarine
version in cursor because I wanted to
hide the fact that I was using GPT5 and
lower the likelihood I would
accidentally leak that when I was
streaming. The reason I couldn't was
hilarious. They had custom prefixed GPT5
to have the new behaviors in cursor and
they hadn't done a custom prefix for
nectarine with the same thing. So you
can see in cursor when I go to select
the model it has the little brain icon.
That's because it's using the max mode
which is a feature in cursor that it
knows what the model's capable of. When
I put in nectarine it wouldn't have
that. So I couldn't use that version in
cursor at all. So I was using the GBD5
reasoning alpha cisprompt new reasoning
effort for the vast majority of my
testing inside of cursor. I did switch
to this for a bit and it was I didn't
use it enough to have a really strong
opinion on how it differed from the CIS
prompt version. But again, these are the
same model. This one just has a custom
endpoint that has a different system
prompt being inserted. All three of
these are the exact same model. This and
this are different endpoints for the
same model that returns the exact same
thing. And this adds a system prompt,
but is effectively the same otherwise.
Here is where things get really fun.
There's one other model that is the
exact same as all of these GPT5. This is
the exact snapshot that they released.
They have no reason to lie about this
and I've been able to verify it to the
best of my ability externally as well
because OpenAI was kind enough to reopen
the GPD5 reasoning effort version for me
to use. So I spent the last week no
lifing this testing this endpoint
against the standard GPT5 and they
behave almost identically. At first I
was confused though because of a thing
that a lot of people have been assuming
which is that when I was using this
obviously this is the equivalent of GBT5
high fast. Duh. Obviously I was using
the smartest version. Obviously OpenAI
has us using the smartest version. Nope.
It turns out, and this was a fun thing
to learn through this debugging and also
talking to OpenAI. Crazy what happens
when you usually talk to the people,
this is the equivalent of standard GPT5,
not the fast version, although it was
effectively the fast version cuz I'm an
entirely different queue. But more
importantly, it's not the high version.
I was not using the high reasoning
effort, high juice version of GPT5 at
any point during any of my testing. The
first time I got to try GPT5 High was
the day that GPT5 came out. This was a
big surprise to me because I even had
fallen for this that I just blindly
assumed the thing we were testing in
cursor. It must be high. There's no
other reason that that version would be
smarter and this version would be
dumber. But that's another one of the
hot takes that I've been developing. The
GBT and cursor today is not dumber than
the one I used. It just sucks more when
using it. We talk about Gro 4. You guys
have seen my video on Gro 4. We've all
seen the benchmarks from Gro 4. It's one
of the smartest models ever made. It
regularly outperforms almost everything
else on every benchmark. It is clearly
one of the smartest models ever made and
I avoid using it entirely. Not just
because it's super expensive, not just
because it's super slow. Those are
enough reasons in and of themselves. But
the main reason I don't use Grock for is
because it sucks to work with. What I
mean by that is it tries to go 200 IQ on
everything. It loves to overthink basic
stuff like what skateboard trick is
this? It'll literally generate 10,000
tokens when asked to name a skateboard
trick when other models will do like 15.
The fact that it is so smart almost
works as a detriment to it because it
tries to 200 IQ everything. It tries to
overthink everything and it takes
forever to actually do the tool call
that supposedly it was trained on and it
also because it was trained on tool
calls will hallucinate them all the
time. I cannot tell you how many times
we've seen Gro 4 dump fake syntax for
doing a search in a chat response
because it's ignoring the actual search
tool we've provided it because Gro 4 was
trained so aggressively on all these
different things. It actually kind of
sucks. I would never have predicted that
would be the case, but it is. This is an
interesting realization to have because
that's kind of what's happening with
GBT5 right now since the model behaves
so differently from others. Not in the
sense that it's like really weird and
quirky. Honestly, kind of the opposite.
It's much less so than other models. It
does what you tell it. For the most
part, because of that, a lot of the
tools that are built to kind of sandbox
the models, to not touch things they're
not supposed to, and to guide them
really carefully with the right tools,
those things don't work as well with
GPT5. And a lot of the existing tools
have not been very good when plugged
into GPT5. Some amount of this is the
new harmony response format standard.
Some amount of this is because we built
everything around what Claude wants. And
some amount of it is just incompetence
and weirdness and GPT5 being a strange
model to work with. But the state it was
in when I tried it was admittedly a bit
buggy, but was really good when it was
good. And little things like its
willingness to use a to-do list was
quite nice. I have not seen GPT5 use a
to-do list since my early testing. And
that kind of frustrates me because it
was a good experience and it's not
available anymore. But this is just
touching on the problem because a lot of
people who didn't use cursor had a
really bad experience with G55 as well.
Let's say this is the model. We don't
know if it's good, bad, or else. We just
have this thing that exists. How do we
use it? You can hit the API directly and
get a response, but the harsh reality is
that almost nobody does that. The way
most people experience GPT5 is through
another layer. Those layers can be
things that we're all familiar with.
Could be something like chatgbt.com.
Could be something like T3 chat or it
could be something like an AI code tool,
the one that most of us will be using
cursor. So if we are hitting these
layers, we're not hitting the model
directly. We are going through
chatgpt.com
going to the model through that and then
getting a response that way or we're
hitting T3 chat or we're hitting cursor.
When someone sees a new model drop, they
don't rush to go hit the API and run
benchmarks against it. That's what nerds
like me do. And when we did that, it was
really impressive. GBT5 still benches
exactly the same as it did when I was
testing it early. The difference here is
that when I'm testing it ahead of time,
I am throwing it through whatever I can,
seeing how it performs, seeing how it
behaves, and I'm also using both
chatgbt.com and cursor in a different
state than they'll be in once they come
out. Because when I was testing it,
these parts were still very early. They
were figuring out the auto router. And
honestly, the little bit of testing I
did on chatgbt.com with GBT5 was not
very good. And I stopped using it almost
immediately. But my experience in cursor
was pretty good. So most of my time
using GBT5 was hitting the end points
for benchmarking and testing and using
it in cursor. I didn't plug it into T3
checks. I didn't want to leak the fact
that I had early access. So I was almost
entirely using this for coding and
agentic work and benchmarking tools. I
was not using it as my daily chat model.
I did go to chatgbt.com and ask it some
hard questions and get surprisingly
decent answers here and there, but
whenever I used the canvas, everything
fell apart. Supposedly, they fixed most
of that. I was not having a great
experience on the website. Overall,
though, some amount of that's cuz the
chatbt site's kind of trash. Some amount
of that is because the auto router is
weird. Some amount of that is because
they didn't have the reasoning model as
the default version on the website.
There's a lot of reasons, but I was
having a very good experience on this
side. And now that we've shipped it in
T3 Chat, a lot of people are saying that
they didn't realize how good the model
was until they tried it in T3 Chat
because we don't have it gimped at all.
We don't have an auto router. You pick
which version of GPT5 you're using. But
this is where the problem comes. As much
as I love T3 Chat and as big as it is,
it is not one of the biggest sources of
use for new AI models. It's surprisingly
big. Like if we go to open router, look
at my favorite model, Gro 4,
you'll see that we're one of the top
users of Gro 4. We're almost as big of a
Gro 4 user as the open router test chat
room. Pretty nuts how big Grock 4 use is
on T3 chat. Big part of why it's such an
expensive service to run. The point I'm
trying to make here is that we do a lot
of throughput, but it's nothing in
comparison to where most people live.
And this is also for a relatively
obscure and not actually nice to use
model Grock 4. So not necessarily
numbers that mean much. Being fourth
place on an obscure model through a
wrapper layer like open router shows
that we are not the popular way to use
GPT5. So the vast majority of people
especially the people who are on
Twitter, the developers who are ship
posting and have strong thoughts about
this, these are the two surfaces that
they went through. And the problem with
layers like this is one of the most
classic settings in the history of time.
A chain is only as strong as its weakest
link. It does not matter how good the
GPT5 model is if the layers that people
are accessing them through are garbage.
And this is why I think GPT5 had such a
rough roll out. I don't actually think
much of it had to do with the GPT5
models at all. I think the vast majority
of what made this roll out suck is the
chat GBT site being absolute [&nbsp;__&nbsp;]
garbage and the auto router being a
terrible experience. They even admitted
that for the first couple days the auto
router was failing and sending every
request to the scoped down reasoning
free version and just getting [&nbsp;__&nbsp;]
answers as a result. And that lines up
with what I was seeing people posting
especially in comparison to T3 chat
where we were hitting the API directly.
Yeah, this is a post from Sam Alman on
Reddit when they did an AMA about the
new models. GBT5 will seem smarter
starting today. Yesterday we had a SEV,
which is an outage, an incident, and the
auto switcher was out of commission for
a chunk of the day and the result was
that GBT5 seemed way dumber. I think it
was more than that. I think that the
reason it went down is that they were
trying to fix it because they realized
how bad it was performing and that
resulted in bad experiences for a lot of
people trying it. And I can fully
sympathize with why most people who go
through this would assume the model's
bad. You see a bunch of people talking
about how incredible and magical this
model is. You see that they're in this
launch video about the new model. You go
to try it yourself. So you go to
chatgbt.com and you try it and it gets
the strawberry question wrong. You ask
it how many Rs are in strawberry and
it's incorrect and you laugh and you go
try it in cursor because everybody's
saying it's good there. And then you try
it there. It locks up a whole bunch.
Gives you a [&nbsp;__&nbsp;] answer after 5 minutes.
You decide everybody's hyping this
things, they were paid to do it, and
then you move on with your life. That
makes a ton of sense to me because
you're not actually testing the model in
any of those scenarios. You're testing
the chatgbt.com implementation and
you're testing how cursor has set up all
of the tools, system prompts, and custom
everything as well as exposing all of
this through the UI. And the way Cursor
implements GPT5 is frustrating. I'll say
that at the very least. a problem I
regularly run into when I use it. And
let me just find a video where I have
this quick. So here I was doing
comparisons between five and the custom
reasoning version. You can see it's a
quick time video. So it's not the most
easy to see thing in the world, but down
here I had the GBD5 reasoning new
reasoning e effort piece. And here I had
standard GPT5. The response quality in
the outputs was basically the same, but
a bug that would happen on both. At
first, I thought it was just main GPT5,
but it happens in both. This isn't done
yet.
The bug is that when it's still going
and you see the stop button there, it's
working. It's generating a file here.
It's writing the tool call to edit the
file, but cursor doesn't indicate this
in the UI anywhere. So, it feels like it
just froze. These little details of UX
result in the experience you have with
GPT5 feeling significantly worse. the
actual using the model feels bad. And I
still feel this now. It is very, very
frustrating when I'm trying to use GPT5.
I regularly think, "Oh, it must have
frozen." And then see the stop button is
still there and I realize, "Oh, it's not
frozen. They just don't have a UI state
for when it's in this spot." All they
have to do is put a little thing
underneath the text here that says
reasoning or generating to make the
experience feel quite a bit better. But
they're not doing that. And the result
is that it feels bad compared to models
like Cloud Force Sonnet or Opus or even
new stealth models like Sonic. The
experience using them feels better
because they generate token per token in
a way that's a little more visible. But
more importantly, the UI shows what
they're doing at all times, even if it
shows silly things like what I just
found and ended up posting when I filmed
my last video. I had Sonic building a
benchmark tool just as a demo. and it
thought for six seconds and it thought
the index. That's all it thought. This
is silly and dumb, but at least you see
it making progress. You see loading
states, you see it working. GBT5 takes
way longer. It's not a fast model. It
generates tokens quickly, but it also
does a lot of work and will do a lot of
chained tool calls and back and forth.
So, it takes longer to complete a task.
It's way more likely to complete the
task well, but it takes a longass time
to do it. and the UI has not caught up
to that slowness. The cursor experience
does not work well alongside GPT5 and
it's easy to feel like things are
failing and crashing and sometimes they
do. I was having a problem where it
would fail to use the edit tool over and
over again. I ended up working with the
guys over at Cursor getting them some
traces and they're trying to fix it.
They might have even fixed it by now.
But all these little details added up to
a lackluster experience. Some of these
things were happening during my early
testing. It felt like it was less common
than it was after. But the result just
wasn't great. And when you tried
switching over to using Gvt5 high, it
would be even slower and these problems
wouldn't go away. The thing I'm trying
to highlight here is that the surfaces
that we use to access the model being
the chat GBT site and cursor result in
the thing that we actually experience
being worse than it would have been
hitting it directly. One other
significant portion of this is how
receptive Gvt5 is to system prompts
because the model does what you tell it
to do, nothing more, nothing less. It
has some of the most impressive results
on my benchmarks, in particular the
benchmarks around things like
snitchbench where if you don't tell it
to snitch, it won't snitch. You tell it
to act boldly and in the interest of
humanity, it might snitch. But if you
don't give it specific instructions to
it, probably won't. And if you don't
tell it to act boldly, you just give it
a task, it does the task without
hesitation. And when I ran it against
the anthropic benchmark for this, same
deal. The model does what you tell it.
It doesn't try to work around it. It
doesn't touch things it shouldn't for
the most part. It'll edit a read me here
and there, but beyond that, I'm
regularly surprised that GBD5 will sit
there, think for 5 minutes, edit two
files, total five lines of code, and
solve the problem surprisingly well. But
that doesn't feel great. It certainly
doesn't feel like super intelligence. It
actually feels pretty slow and bad UX-
wise. I've never had a model where I'm
spending more time doing other [&nbsp;__&nbsp;]
This is the first model where I bothered
like relearning work trees so that I
could have multiple tasks running in
parallel because it's too slow to just
sit there and wait for. So you go to
chatgbt.com and you get a dumber model.
You go to cursor and you get a super
slow model that fails tool calls and
hangs all the time in the UI. What the
[&nbsp;__&nbsp;] Why would anyone use this? The
harsh reality is just that the model's
really good. Like I if I've learned
anything through all the time I put into
this testing over the last week is that
the model is actually [&nbsp;__&nbsp;]
incredible. I know it's controversial.
People are going to call me a page chill
or whatever. After how hard I just shhat
on chatbt.com. I hope you guys
understand I am not being paid a [&nbsp;__&nbsp;]
scent for any of this. The GD5 model is
so good at various things. It's not like
it's way smarter about hard code
problems than Opus. It goes down weird
tangents less often. It's way better at
UI design. I've done a bunch of
comparisons now. It is the best UI model
ever. It just does tailwind better. It
does gradients more tastefully. When I
give it a UI task, it makes something
that looks and feels significantly
better than any other model I've ever
used other than the Horizon models,
which I at this point I'm pretty sure
are the GPT5 non-reasoning versions. The
model is really good. And one of the
funniest signals I have for this is that
all the people I'm talking to at Cursor
use GPT5 as their daily driver still.
Pretty much entire Cursor team, as far
as I know, is still all in on GPT5. Even
with these UX failures that I hope
they're fixing really aggressively right
now, they prefer this model. It would be
funny when I would talk to my friends at
Cursor and be like, "Yo, here's a list
of the problems I'm having using Cursor
with GPT5. This is really frustrating."
And the response from I'll just say Eric
like I remember the first time you
responded to it was like this is crazy
because GBT5 is still the model I choose
every day. It feels so much better than
Sonnet. And I thought about it and
realized he's right. I do not like using
Sonnet or Opus anymore. The UX is
slightly better in the editor. So like
the way it does tool calls, the way it
spins up the to-do list, the way it goes
through tasks and completes them. Sonnet
is absolutely a workhorse and it's built
well into lots of different things.
There's a reason that it's the model
that's used for almost all agentic work,
but GPT5 is better. I have not went back
to a clawed model for anything other
than testing, either to compare it with
GBT5 or to look into how it's doing tool
calls and all of these types of things.
And something else I've been doing, and
I've been doing this a lot, whenever I
have a problem that GBT5 can't answer, I
go to try it in Opus. And there's not
been a single [&nbsp;__&nbsp;] time for me that
Opus got even vaguely close to answering
a thing that GPT5 cut it. That's fun.
Allow chat GBT to store data in
persistent storage. Huh. Someone's
trying to do more local first. Wonder
what's up there. As you guys all
probably heard, when GBD5 dropped, I was
in Vegas for Defcon. The reason I go to
Defcon is this particular set of
challenges called Gold Bug that I'm
addicted to. I'm not going to go too in
depth. You can watch my videos about it
from last year with the TLDDR. We get a
bunch of puzzles that can be solved in a
varying set of ways. You have to find a
12 character phrase in each one. And the
majority, actually, I think the entirety
of them are pirate themed this
particular year. The puzzle I spent most
of my time on was this one, smugglers
manifest. The puzzle is this one PDF.
This PDF says at the top, we know they
smuggled the emerald out of the port,
but how? It's meant to be a shipping log
of items that went in and out of
different ports, where they came from,
where they're going to, how much they
weigh, and a description of the item.
And the expectation is that we go
through this and find the hidden code
somewhere in it. No one's going to try
this, so I don't feel bad spoiling it.
If you do, just skip till you don't see
this on the screen anymore. The solution
ended up being you have to realize that
these IDs on the side are an ADFGX
cipher because there's a hint at the
bottom. This all the trade the Germans
intercepted. Germans in World War I used
this particular bad cipher. And we have
to find the path the emerald took
between different things. Combine all of
the IDs. Then we have to find the key
word that we can use to transpose it to
then hopefully generate the right
answer. This was chaos in particular.
Finding the path was chaos. And I was
trying to figure out all of the weight
discrepancies that were likely the place
where the emerald was being hidden. So,
one example is here where there is a
small carved box with a foam lining
that's supposed to be 2.5 O, but it's
weighing 2.6. This comes up two other
times where it says 2.5 and it is 2.5.
But this one particular place it says
2.5 and it's actually 2.6. So, that is a
discrepancy.
There are other suspicious ones too,
like uh here wrapped in banana leaves.
It doesn't say what it is. It just says
wrapped in banana leaves and it weighs
Obviously, it's because 2.5 2.5 o,
right? Cool. I was tired of Google
searching how much things weighed and
doing. So, I threw this at GBT. What
items in the manifest don't match their
weights? Here, it called out that these
three IDs say the box is 2.5 O. Most of
them weigh 2.5, but it does say here the
listed weights read like pounds while
the description says 2.5 O. Not the best
response, but it said here it reads like
pounds, but the description says that
annoying. I could not get [&nbsp;__&nbsp;] Claude
Opus to do anything other than misassume
that it's supposed to be pounds. Like
here, 205 is 0.156, not 2.6. 205 O is
not 2.5 pounds. It insisted on this
point. These discrepacies suggest either
data entry errors or potentially
deliberate falsification of weights
possibly to hide contraband. It is
assumed the weights are in ounces, not
pounds. I don't know why you'd assume
otherwise. You're absolutely right.
I want to die. Not helpful at all. This
is my experience with Opus. I'm not
happy with the answer from JGBT.
I try GBT5 and I don't get a thing that
I'm content with. I go back and I ask
Claude, and by the way, I don't think
it'll show it here. During this
generation, it ran out of thinking
budget and asked me to hit continue to
let it keep going. I had such a bad I
tried so hard to get useful info out of
Opus for these puzzles and it just it
cannot help at all. Here is me playing
with it in T3 chat using various models.
I think this one, yeah, this was cloud
for sonnet reasoning and it did better
with figuring out that 2.5 and 2.6 don't
line up. To be fair, I didn't give it
the PDF. I gave it a JSON dump instead.
But the funnier thing here is after it
figured that all out, it just
hallucinated the rest. It ignored the
fact that the IDs are a cipher and
concluded that one of the items was the
secret. So, the answer must be secret
locket. It just hallucinated answers.
This is probably my favorite difference.
ChatGBT would help me with the things it
thought. ChatGBT was what told me to
look at the ADFGX cipher. ChatGpt was
what told me that it was based on a
specific movie, Romancing the Stone.
Chat GPT helped me figure out pieces. It
never tried to give me the answer. It
tried generating code to calculate the
answer and it failed every time. But at
least it gave me the pieces I needed to
figure things out. Anthropic will say
something obvious and then hallucinate
an answer. Almost every time I tried a
[&nbsp;__&nbsp;] anthropic model to help me with
this, it would just delusionally make up
answers. Or even better, when I did it
through their website, it would write
code, run it, get no answer, lie, and
then hallucinate an answer. So here, it
made an interactive artifact to run code
to try and decrypt the hidden thing. So
this why I was just telling it it's an
ADFGX cipher. It's referencing this
movie. The 12 character string with no
whatever else. Write me code to solve
this.
Created the JavaScript called out
suspicious items. What's really funny is
the hall appendent wasn't the item
because the emerald weighs too much for
it to have weighed that little there.
And then after doing the ADFGX cipher
code, it didn't run the code. It just
came up with a bunch of potential
answers. The most likely answer given
the context is probably emerald stone.
No, it [&nbsp;__&nbsp;] wasn't.
This has been my experience using
Claude. It almost starts down a
semiirect path and then hallucinates its
way to hell. In contrast, here is me
handing a different puzzle admittedly to
chat GPT on the chatbt site. I was using
pro cuz I was just curious. I had not
used Pro at all up until this point. It
reasoned for 15 minutes and it went kind
of ham. It wrote a bunch of code. It did
its best to try and solve the puzzle,
but it also once it ran the code, it got
nonsense text and it realized that
that's not the solution, but also would
do this randomly. It would just say
thought and then not finish the work it
was doing. This is the key difference.
It gives us observations. The bottles
are numbered in reverse order because it
would go scan the page. The shanty
pattern repeats three swigs ahead, four
cups, possibly take every fourth bottle,
move three bottles forward in the
lineup. It has theories, likely
mechanism. This feels a lot like the
Josephus problem. Start a given bottle,
skip forward, three steps.
It gave real useful information. And
instead of hallucinating an answer, it
offered to write JS code to try and
solve the problem. And to be clear,
neither of these answered it. But I
almost think that's what makes it more
interesting. Neither knew the answer
because this is a novel and very
difficult problem. A chat GPT in GBT5
would give me useful pieces to work
towards it, not make [&nbsp;__&nbsp;] up and would
ask me if I it should proceed with a
theory that it proposes. Whereas with
Claude, it will almost start being a
little bit right, quickly ignore the
context I gave it, go down some
absolutely chaotic path, write a bunch
of code, not use the code, and then
hallucinate an answer.
This is my experience. So, I'm sure that
the clawed models are better for
something. I still think they're better
at like personality. Like if I'm trying
to talk through a personal problem with
a model, which you shouldn't do, but
let's be real, we all occasionally do
it. I find the anthropic models are
nicer to talk to about like relationship
troubles and [&nbsp;__&nbsp;] like that. The chatgbt
models actually get work done for me
now. And once again, to be clear, that
was not the case before. There was no
other OpenAI model that was useful for
me for work in that way. And to be very
very clear, this is not how I felt about
any previous OpenAI model. They were not
good enough at tool calls. They were not
good enough at doing things in steps.
They were good at thinking and coming up
with an answer that seemed right. But
this much more deliberate behavior is a
thing that I find is very new and very
nice with GPT5. And even then, the
chatbt site was kind of annoying. I was
picking pro mostly to keep it from being
autoed out of the smart models and
towards the dumber versions. I found so
many bugs on the chat GPT site when I
was doing this set of puzzles. It was
hilarious. It was also just a generally
good way to test the different models
and the different endpoints and a lot of
different things. I probably wrote 5,000
lines of code in cursor throughout these
challenges. One of these puzzles we
spent a lot of time on a single gold
coin. It had a really jank 3JS viewer by
default. Like really jank.
And I rewrote it to make it easier to
see. and my rewritten version was like
like I know my way around React 3 fiber
and 3JS. I don't remember all the
specific like camera techniques that I
need to use and [&nbsp;__&nbsp;] So I vibe coded a
lot of it with basic prompting like go
in this direction, make these changes,
don't touch this, it's fine. And after
like 20 to 30 minutes of vibe coding
could make something that looks and
works pretty damn well. I'm actually
very happy with how this came out,
especially in comparison with the
original, which I don't have the link
for anymore,
but they liked mine so much that now it
is the official one. When you click the
link on the CryptoVillage site, it
brings you to my solution. It's not
really a solution. It's a new viewer,
but I was really proud of that fact and
that I could vibe code that out with
GPT5 in 20 minutes while in parallel
solving other problems. I bring all of
this up not because the ultimate way to
test a model is by doing really obscure
puzzles at Defcon. I bring this up
because I'm trying to showcase my lived
experience in how different it is from
how I see others describing the new
models. And again, to be clear, it
didn't solve the problem in any of these
scenarios, but it helped a ton with me
getting towards the answer. And that is
not an experience that I had had before.
I never felt like a model was a coworker
in that way. I can't tell you how many
times when I'm pairing, I'm going back
and forth with a person writing code and
neither of us are right. Neither of us
know the solution, but we're iterating
on each other until a solution comes
out. And this model can do that. I
haven't even touched this article yet. I
want to go through this quick though cuz
I'm excited to see how others feel.
Everyone agrees the release of GBD5 was
botched. Everyone can also agree that
the direct jump from 40 and 03 to 5 was
not of similar size as the jump from
GPT3 to GPT4. Okay, from 03 to GBD5, I
agree. But from 40 to 5, I heavily,
heavily disagree. GBD 40 was a garbage
model, and I'm so happy it's effectively
dead now. GBD5 represented the release
of at least three distinct models. Five
fast, five thinking, and 5 Pro. And at
least two or likely all three of these
are state-of-the-art within their
classes along with the GPT5 auto. To be
fair, GPT5 auto is not a thing on the
API. It's just a thing in the app. I
would not measure that for much of
anything. problem is that the release
was so botched that OpenAI is now
experiencing a reverse DeepSeek moment.
All the forces that cause us to
overreact to R1 are now working against
OpenAI in reverse. This threatens to
give DC and its key decision makers a
false impression of a lack of AI
progress, especially progress towards
AGI, which could lead to some very poor
decisions. In January, when R1 came out
from deepseek, we had the Deep Seek
moment where everyone panicked that
China had caught up. R1 was a good
model, but only an ordinary good model,
substantially behind the frontier.
We had the deep sea moment because of a
confluence of factors that misled
people. The $6 million model narrative
gave a false impression on cost. They
offered a good clean app with visible
chain of thought. Eh, it helped, but not
as much as people seem to think. The new
style caused an overestimate of model
quality. That I agree with. I still
think V3 was better than R1. The timing
was impeccable, both in order of model
releases and within the tech tree. Yes,
susting and other steps were skipped,
leaving various flaws, and this was a
pure fast follow, but in our haste, no
one took any of that into account. There
was a false impression of momentum in
stories about Chinese momentum. The
always insists on open models win thing
definitely amplified it and the stock
market was highly lacking in situational
awareness suddenly realizing various
known facts and also misunderstanding
many important factors. Now five is
having the opposite. It's evaluated as
if it was scaling up compute in a way
that it doesn't. GBD5 definitely cost a
lot of money to make but it didn't
necessarily cost significantly more.
Offered a poor initial experience with
rate caps and lost models and missing
features. A broken router complaints
about losing 40 Yep. The new style and
people evaluating five when they should
have been evaluating five thinking
caused an underestimate of model
quality. Again, the web app hurt them a
ton. The timing was directly after
anthropic and the previous releases had
already eaten the most impressive recent
parts of the tech tree. So, gains
incorrectly look small. In particular,
gains from a reasoning model and from 4
to 4 are being ignored when considering
the four to five leap. Yep. GBT5 is a
refinement of previous models optimized
for efficiency and is breaking new
territory and that is not being taken
into account. I agree here in
particular. GBT5 isn't great because
it's so much smarter. It's great because
they refined a lot of [&nbsp;__&nbsp;] out of
other models and it behaves much better.
Paul's impression of hype in a story
about a loss of momentum. Yep. Open eyes
flailing crowd and the stock market was
smart and shrugged it off. Unlike R1,
Five Thinking, and 5 Pro are clearly the
current state-of-the-art models of their
classes. And 5 Auto is probably
state-of-the-art at the level of compute
that it uses. We don't know because they
hide the compute usage, but yeah,
OpenAI's model usage was way up after
5's release, not down. The release was
botched, but this is a very obviously
good set of models. Apparently, DC is
assuming that it is a failure and that
AGI is no longer a worry. Tons of people
who barely use AI are asking me about
the past week as their AI policy friend.
I don't think FI was a failure. person
tells me they aren't allowed to use LMY
at job ABC because regulatory
considerations. So they use LMZ at home
because that's what they started to use
first and they don't have much
experience with a different one. So what
happens when another lab releases a
model that surpasses five? Narrative can
quickly change from AI's hitting a wall
to open AI has lost the mandate of
heaven and it's now shifted to one of
these other places. It's a very valuable
time for someone else to drop a model.
Absolutely. Even if a new model performs
slightly worse than five, the amount of
positive press the next model drop is
going to have is going to be insane. The
thing that makes me think we actually
might be approaching AGI, I don't like
the term, I don't like the phrasing, I
don't like talking about this even. This
is the bench I think about. This is how
long can an LLM work before it has a
higher than 50% failure rate. And GBT5
is capable of going for over two hours
before it starts to hit a higher than
50% chance failure rate. That is very
impressive. That is what's changing now
is how long can it work for before it
hits hard walls and a human has to come
in and steer it in the right direction
again. GBT5 has hit groundbreakingly
high numbers here. This is the chart
that in many ways shows the actual
progress we're making as a field, not
some benchmark on how smart it is, how
close it can get to 100%. You're like,
where's Gemini? Here, which lines up for
me. Gemini 25 Pro does not run for a
long time very well. It's also really
fast, which hurts its numbers here
because it it like generates tokens
quicker. If you were to measure how many
tokens can it generate until it fails,
2.5 Pro would probably look a lot
better. But how long can it work
independently for? That's the question.
And then other silly things like this
wonderful one. GBD5 was able to complete
Pokemon Red with 33% of the steps that
other models had when doing it. 18,000
steps is what it took for 03 to complete
Pok√©mon Red. GBD5 did it in 6,500.
Actual work is the magic thing. And this
is a pretty good question from Finger. I
don't get the time thing. Why does it
matter? It's just more data, not
necessarily smarter. This matters a ton
because it's how much work can you get
done. If imagine you have two engineers,
they are roughly as intelligent as each
other. One of them can work for 15
minutes before having a 50% chance they
[&nbsp;__&nbsp;] up. And one of them can work for 2
and 1/2 hours before they have a 50%
chance of [&nbsp;__&nbsp;] up. One of those
engineers is a significantly better
engineer. It doesn't matter if they are
the same intelligence or if you gave
them a test, they would get the same
number of answers correct. The one that
can independently work for longer is
obviously the better engineer. And the
examples they have on the side here make
a lot of sense too. For these shorter
term models, they can find information
on the web. For something that can run
for an hour, it now can do something
like train a classifier. As it gets
more, it can now probably fix bugs in
small Python libraries agentically
behind the scenes. This is where like
cloud code and stuff come in. As you
break the hour 30 mark, now you can go
scrape records from websites that have
anti-bot protection and collect the data
and do things with it. Once you break
the 2-hour mark, you can start
exploiting libraries. Like if there's a
buffer overflow in some random lib, you
can tell that to the model and it can go
experiment for two plus hours by itself
and figure things out. That's the
difference. It doesn't matter who's more
or less smart. A really smart person who
can only work for 10 minutes and a
pretty smart person who can work for 3
hours. That's the key. It's task
duration for humans to be clear. But
this is more how long can it work
independently for. The length of tasks
that AI can do is doubling every seven
months. This is the magic chart. The
progress we're making here in particular
is nuts. And when you compare that to a
chart like this, it doesn't look very
impressive. Like, oh, Gemini 2.5 Pro did
a really good score on this set of
benchmarks. GBT5 high isn't that much
higher. Oh, I guess we're stalling out.
Not that simple.
Oh, all of these are different links.
That's hilarious. These are all links
about how we're hitting a wall with GBT
and they're very dumb. People
complaining the vibes aren't good, which
means the model's bad. People expected
something totally new, said Thomas Wolf,
co-founder and chief scientist at
Hugging Face. And here we didn't have
that. True, we didn't get something
totally new, but again, that was OpenAI
botching the rollout, using the name
GBT5, and having made many incremental
releases since 4, like 4013. They hit
the classic notes. This whole article is
really good. I recommend reading the
whole thing, but this part in particular
is key. OpenAI rightfully thinks of
itself as essentially multiple
companies. They are an AI frontier
research lab, but also a consumer
product company and a corporate or
professional product company as well.
They're looking to be a hardware company
at this point, too. Most of the
customers want to pay $0, at least until
you make yourself indispensable. Most of
the rest are willing to pay up to 20
bucks a month, and they're not
interested in paying more. You want to
keep control over this consumer market
at Kleenex or Google levels of
dominance, and you want to turn a
profit. So, they are prioritizing making
things like the auto model and auto
router cheap and smart for the price
instead of the smartest possible thing.
It may not have been OpenAI's
intentions, but with the launch of GBT5
is making clear is that the nature of
the AI rice has changed. Instead of
merely building shiny bigger models, a
researcher said that AI companies are
slowly coming to terms with the fact
that they're building infra for
products. Yes, this is key. Everybody
who's been mad at me for how I talk [&nbsp;__&nbsp;]
at Grock, that's cuz they're not doing
this part. They're not building useful
tools for us to build around. The thing
I'm about to say is going to probably
hurt me in my business. There is not
much room for innovation left in chat
apps models. Getting way smarter doesn't
meaningfully improve the experience you
have on T3 chat or chatgbt.com. They're
already really good. Maybe it gets a
hard question right slightly more often,
but that's not a massive improvement.
The thing that's changing is how complex
of tasks and how long of tasks these
models are able to complete themselves.
That's a huge change and that's not one
you're going to see on a random chat
app. And thankfully a lot of companies
are realizing this and they're working
on the infra side more and more now.
Google just put out the ability to hand
a link to a model either in the chat app
or even more importantly over the API
and it will be able to go search the web
page, scan it, bring it into context and
use it. That's awesome. That is infra.
That is solutions to problems we have as
we build using these tools. That's where
the real battlefront's going to be is
the layer between the model and the rest
of the [&nbsp;__&nbsp;] world. It makes sense
that as AI gets applied in a lot of
useful ways, people would focus more on
the applications versus the more
abstract ideas like AGI. There's a key
point in this article that honestly
might end up being a video of its own
someday. I don't want to harp on it too
much now, but this does have a chance of
massively screwing things up because it
could fool the government into ignoring
the possibility of AGI actually
happening. Analysts say that with AGI no
longer considered a risk, Washington's
focus has switched to ensure that
US-made AI chips and models rule the
world. Open AI badly messes up the vibe
around the five launch and then
eventually US gives up on export
controls to China. Yeah, I think that
covers everything I have to say here.
This was quite an article. This has been
quite a drama. The points I wanted to
emphasize are that the model I was using
is the same as what you're using now. It
is the medium reasoning version of GPT5
over the API. All the different stuff I
tested were snapshots of that. And the
main ones I used which were these three,
they are the exact same snapshot that
you use today over the API. The problem
is that nobody uses the API. They use
surfaces like chat GPT and cursor that
made the experience feel much worse for
various reasons. OpenAI [&nbsp;__&nbsp;] this up
with the chat GPT side. They screwed up
really bad with how the router rolled
out and how they deprecated old models
which made it feel awful. That is their
fault. They need to own that. But also,
Cursor made a bunch of lastminute
changes trying to make the experience
for chat GPT and GPT5 in Cursor better
and failed. And there's a lot of UX
failures in Cursor's implementation
right now that are still pervasive. I
ran into one earlier when I was testing
it like today. But despite all of that,
I still think Five is an incredible
model. It's still the one I use for all
the work I do. It's still the one I talk
to in T3 chat. Okay, I use 5 Mini quite
a bit more because it's really, really
good and insanely cheap. And honestly, I
might have to do one more GPT5 video,
but not about GPT5 because 5 Mini and
Nano are criminally underrated models.
They are so insanely good for the money
that I'm amazed people aren't talking
about it more. And if you think that's
interesting, let me know and that'll be
my next video. Hope this was useful to
all of you guys. Sorry again for all the
drama. I hope you understand that I am
trying my best to be as transparent as
possible about all of this chaos. I
don't have that much more info than you
guys and I'm doing my best to share the
little bit I know and have. Hopefully
this helps you better understand what
went wrong and why the perception is as
bad as it is and where this model
actually lands. Let me know what y'all
think. Until next time, peace nerds.