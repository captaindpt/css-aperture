**Transcript extracted from YouTube video**

Kind: captions
Language: en
I’ve had a lot of discussions on my podcast&nbsp;
where we haggle out our timelines to AGI.&nbsp;
Some guests think it’s 20&nbsp;
years away. Others 2 years.&nbsp;
Here’s where my thoughts lie as of July 2025.
Sometimes people say that even if all AI&nbsp;&nbsp;
progress totally stopped, the systems of&nbsp;
today would still be far more economically&nbsp;&nbsp;
transformative than the internet. I disagree.&nbsp;
I think the LLMs of today are magical.&nbsp;
But the reason that the Fortune 500 aren’t using&nbsp;
them to totally transform their workflows isn’t&nbsp;&nbsp;
because the management there is too stodgy.
Rather, I think it’s genuinely hard to get&nbsp;&nbsp;
normal humanlike labor out of these LLMs.
And this has to do with some fundamental&nbsp;&nbsp;
capabilities these models lack.
I like to think I’m "AI forward"&nbsp;&nbsp;
here at the Dwarkesh Podcast.
I’ve probably spent over a hundred&nbsp;&nbsp;
hours trying to build these little LLM&nbsp;
tools for my post production setup.&nbsp;
And the experience of trying to get them&nbsp;
to be useful has extended my timelines.&nbsp;
I’ll try to get an LLMs to rewrite autogenerated&nbsp;
transcripts for me, to optimize for readability&nbsp;&nbsp;
in the way a human would.
Or I’ll try to get them to&nbsp;&nbsp;
identify clips from a transcript that I feed in.
Sometimes I’ll try to get them to co-write an&nbsp;&nbsp;
essay with me, passage by passage.
These are simple, self contained,&nbsp;&nbsp;
short horizon, language in, language out tasks—the&nbsp;
kinds of assignments that should be dead center&nbsp;&nbsp;
in the LLMs’ repertoire. And they're 5/10 at&nbsp;
them. Don’t get me wrong, that is impressive.&nbsp;
But the fundamental problem is that LLMs don’t&nbsp;
get better over time the way a human would.&nbsp;
This lack of continual learning&nbsp;
is a huge huge bottleneck.&nbsp;
The LLM baseline at many tasks might&nbsp;
be higher than the average human's.&nbsp;
But there’s no way to give&nbsp;
a model high level feedback.&nbsp;
You’re stuck with the abilities&nbsp;
you get out of the box.&nbsp;
You can keep messing around with the system&nbsp;
prompt, but in practice this just doesn’t produce&nbsp;&nbsp;
anything even close to the kind of learning and&nbsp;
improvement that human employees experience.&nbsp;
The reason humans are so useful&nbsp;
is not mainly their raw intellect.&nbsp;
It’s their ability to build up context,&nbsp;
interrogate their own failures,&nbsp;&nbsp;
and pick up small improvements and&nbsp;
efficiencies as they practice a task.&nbsp;
How would you teach a kid to play the saxophone?
You'd have them try to blow into one, and then&nbsp;&nbsp;
they'd see how it sounds, and they'd adjust.
Now imagine if this was the way you'd have to&nbsp;&nbsp;
teach saxophone instead: A&nbsp;
student takes one attempt.&nbsp;
And the moment they make a mistake, you&nbsp;
send them away and you write detailed&nbsp;&nbsp;
instructions about what went wrong.
And you call the next student in.&nbsp;
And the next student reads your notes&nbsp;
and tries to play Charlie Parker cold.&nbsp;
And when they fail, you refine your instructions&nbsp;
and you invite the next student. This just&nbsp;&nbsp;
wouldn’t work. No matter how well honed your&nbsp;
prompt is, no kid is just going to learn how&nbsp;&nbsp;
to play saxophone from reading your instructions.
But this is the only modality we have to ‘teach’&nbsp;&nbsp;
LLMs anything. Yes, there’s RL fine tuning.&nbsp;
But it’s not a deliberate, adaptive process&nbsp;&nbsp;
in the way that human learning is.
My editors have gotten extremely good.&nbsp;
And they wouldn’t have gotten that way if&nbsp;
we had to build bespoke RL environments for&nbsp;&nbsp;
different subtasks involved in their work.
They’ve just noticed a lot of small things&nbsp;&nbsp;
themselves and thought hard about&nbsp;
what resonates with the audience,&nbsp;&nbsp;
what kind of content I like, and how they&nbsp;
can improve their day to day workflows.&nbsp;
Now, it’s possible to imagine ways in&nbsp;
which a smarter model could build a&nbsp;&nbsp;
dedicated RL loop for itself which just&nbsp;
feels super organic from the outside.&nbsp;
I give some high level feedback, and the&nbsp;
model comes up with a bunch of verifiable&nbsp;&nbsp;
practice problems to RL on—maybe even a whole&nbsp;
environment in which it gets to rehearse the&nbsp;&nbsp;
skills that it thinks it's lacking.
But this just sounds really hard.&nbsp;
And I don’t know how well these&nbsp;
techniques will generalize to&nbsp;&nbsp;
different kinds of tasks and feedback.
Eventually the models will be able to learn&nbsp;&nbsp;
on the job in this organic way that humans can.
But it’s just hard for me to see how that could&nbsp;&nbsp;
happen within the next few years, given&nbsp;
there’s no immediately obvious way in&nbsp;&nbsp;
which to slot in continuous learning into&nbsp;
the kinds of models that these LLMs are.&nbsp;
LLMs actually do get kinda smart and&nbsp;
useful in the middle of a session.&nbsp;
For example, sometimes I’ll&nbsp;
co-write an essay with an LLM.&nbsp;
I’ll give it an outline, and I’ll ask it&nbsp;
to draft the essay passage by passage.&nbsp;
And all its suggestions up till&nbsp;
paragraph four will just be bad.&nbsp;
I'll just rewrite every single paragraph from&nbsp;
scratch and tell it, "Look, your shit sucked.&nbsp;
This is what I wrote instead."
And at this point, it will actually start&nbsp;&nbsp;
giving good suggestions for the next paragraph.
But this whole subtle understanding of my&nbsp;&nbsp;
preferences and style will just&nbsp;
be lost by the end of the session.&nbsp;
Maybe there is an easy solution to this that&nbsp;
looks like a long rolling context window, like&nbsp;&nbsp;
Claude Code already has, which just compacts the&nbsp;
session memory into a summary every 30 minutes.&nbsp;
I just think that titrating all this rich&nbsp;
tacit experience into a text summary will&nbsp;&nbsp;
be brittle in domains outside of software&nbsp;
engineering, which is very text-based,&nbsp;&nbsp;
in which you already have this external scaffold&nbsp;
of memory that is stored in the codebase itself.&nbsp;
Again, think about what it would be like to&nbsp;
teach a kid to play the saxophone just from text.&nbsp;
Even Claude Code will often reverse a hard-earned&nbsp;
optimization that we engineered together before I&nbsp;&nbsp;
hit /compact —because the explanation for why&nbsp;
it was made didn’t make it into the summary.&nbsp;
This is why I disagree with something that&nbsp;
Anthropic researchers Sholto Douglas and&nbsp;&nbsp;
Trenton Bricken said on my podcast.
And this quote is from Trenton:&nbsp;
"Even if AI progress totally stalls&nbsp;
(and you think that the models are&nbsp;&nbsp;
really spiky, and they don't have general&nbsp;
intelligence), it's so economically valuable,&nbsp;&nbsp;
and sufficiently easy to collect data on all&nbsp;
of these different white collar job tasks,&nbsp;&nbsp;
such that to Sholto's point we should expect to&nbsp;
see them automated within the next five years."&nbsp;
If AI progress totally stops today, I think less&nbsp;
than 25% of white collar employment goes away.&nbsp;
Sure, many tasks will get automated.
Claude 4 Opus can technically rewrite&nbsp;&nbsp;
autogenerated transcripts for me.
But since it’s not possible for me&nbsp;&nbsp;
to have it improve over time and learn my&nbsp;
preferences, I still hire a human for this.&nbsp;
So even if we get more data, without progress&nbsp;
in continual learning, I think that we will be&nbsp;&nbsp;
in a substantially similar position with&nbsp;
all other kinds of white collar work.&nbsp;
Yes, technically AIs will be able to do&nbsp;
a lot of subtasks somewhat satisfactorily,&nbsp;&nbsp;
but their inability to build up context&nbsp;
will make it impossible to have them operate&nbsp;&nbsp;
as actual employees at your firm.
While this makes me bearish about&nbsp;&nbsp;
transformative AI in the next few years, it makes&nbsp;
me especially bullish on AI over the next decades.&nbsp;
When we do solve continual learning, we’ll see a&nbsp;
huge discontinuity in the value of these models.&nbsp;
Even if there isn’t a software only&nbsp;
singularity, where these models rapidly&nbsp;&nbsp;
build smarter and smarter successor systems,&nbsp;
we might still get something that looks like&nbsp;&nbsp;
a broadly deployed intelligence explosion.
AIs will be getting broadly deployed through the&nbsp;&nbsp;
economy, and doing different jobs and learning&nbsp;
while doing them in the way that humans can.&nbsp;
However, unlike humans, these&nbsp;
models can amalgamate their&nbsp;&nbsp;
learnings across all their copies.
So one AI is basically learning&nbsp;&nbsp;
how to do every single job in the economy.
An AI that is capable of this kind of online&nbsp;&nbsp;
learning might rapidly become a superintelligence&nbsp;
even if there's no further algorithmic progress.&nbsp;
However, I’m not expecting to watch some&nbsp;
OpenAI livestream where they announce that&nbsp;&nbsp;
continual learning has been totally solved.
Because labs are incentivized to release any&nbsp;&nbsp;
innovations quickly, we’ll see a broken early&nbsp;
version of continual learning (or test time&nbsp;&nbsp;
training, or whatever you want to call it) before&nbsp;
we see something which truly learns like a human.&nbsp;
I expect to get lots of heads up before&nbsp;
this big bottleneck is totally solved.&nbsp;
When I interviewed Anthropic researchers&nbsp;
Sholto Douglas and Trenton Bricken on my&nbsp;&nbsp;
podcast, they said that they expect reliable&nbsp;
computer use agents by the end of next year.&nbsp;
We already have computer use agents right&nbsp;
now, but they’re pretty bad. They’re imagining&nbsp;&nbsp;
something quite different. Their forecast is&nbsp;
that by the end of next year, you should be&nbsp;&nbsp;
able to tell an AI, "Go do my taxes."
And it'ill go through all your email,&nbsp;&nbsp;
your Amazon orders, and Slack messages, and&nbsp;
it will email back and forth with every single&nbsp;&nbsp;
person you need to get invoices from, it'll&nbsp;
compile all your receipts, decide what things&nbsp;&nbsp;
are actually are business expenses, and it will&nbsp;
ask for your approval on all the edge cases,&nbsp;&nbsp;
and then will just submit Form 1040 to the IRS.&nbsp;
I’m skeptical. I’m not an AI researcher, so far be&nbsp;&nbsp;
it for me to contradict them on technical details.
But given what little I know, here’s why I’d bet&nbsp;&nbsp;
against this forecast:
One.&nbsp;
As horizon lengths increase,&nbsp;
rollouts have to become longer.&nbsp;
The AI needs to do two hours worth of&nbsp;
agentic computer use tasks before we&nbsp;&nbsp;
can even see if it did it right.
Not to mention that computer use&nbsp;&nbsp;
requires processing images and video,&nbsp;
which is already more compute intensive,&nbsp;&nbsp;
even if you don’t factor in the longer rollouts.
This seems like it should slow down progress.&nbsp;&nbsp;
Two. We don’t have a large pretraining&nbsp;
corpus of multimodal computer use data.&nbsp;
I like this quote from Mechanize’s post&nbsp;
on automating software engineering:&nbsp;&nbsp;
"For the past decade of scaling, we’ve been&nbsp;
spoiled by the enormous amount of internet&nbsp;&nbsp;
data that was freely available for us to use.
This was enough to crack natural language&nbsp;&nbsp;
processing, but not for getting models&nbsp;
to become reliable, competent agents.&nbsp;
Imagine trying to train GPT-4 on all&nbsp;
the text data available in 1980—the&nbsp;&nbsp;
data would have been nowhere near enough,&nbsp;
even if you had the necessary compute."&nbsp;
Again, I’m not at the labs.
Maybe text only training already gives you a&nbsp;&nbsp;
great prior over how different UIs work, and what&nbsp;
the relationship is between different components.&nbsp;
Maybe RL fine tuning is so sample efficient&nbsp;
that you don’t need that much data.&nbsp;
But I haven’t seen any public evidence which&nbsp;
makes me think that these models have suddenly&nbsp;&nbsp;
gotten less data hungry, especially in domains&nbsp;
where they’re substantially less practiced.&nbsp;
Alternatively, maybe these models are such&nbsp;
good front end coders that they can just&nbsp;&nbsp;
generate millions of toy UIs for themselves&nbsp;
to practice on. But, three. Even algorithmic&nbsp;&nbsp;
innovations which seem quite simple in&nbsp;
retrospect took a long time to iron out.&nbsp;
The RL procedure which DeepSeek explained in&nbsp;
their R1 paper seems simple at a high level.&nbsp;
And yet it took 2 years from the development&nbsp;
and launch of GPT-4 to the release of o1.&nbsp;
Now of course I know that it's insanely and&nbsp;
hilariously arrogant to say that R1/o1 were easy—a&nbsp;&nbsp;
ton of engineering, debugging, and pruning of&nbsp;
alternative ideas was required to arrive at&nbsp;&nbsp;
this solution. But that’s precisely my point!&nbsp;
Seeing how long it took to implement the idea&nbsp;&nbsp;
of ‘We should train a model to solve verifiable&nbsp;
math and coding problems,’ makes me think that&nbsp;&nbsp;
we’re underestimating the difficulty of solving&nbsp;
the much gnarlier problem of computer use, where&nbsp;&nbsp;
you’re operating in a totally different modality&nbsp;
with much less data. Okay, enough cold water. I’m&nbsp;&nbsp;
not going to be like one of those spoiled children&nbsp;
on Hackernews who could be handed a golden-egg&nbsp;&nbsp;
laying goose and would still spend all their&nbsp;
time complaining about how loud its quacks are.&nbsp;
Have you read the reasoning traces from o3 or&nbsp;
Gemini 2.5? It’s actually reasoning! It’s breaking&nbsp;&nbsp;
down a problem, thinking through what the user&nbsp;
wants, reacting to its own internal monologue,&nbsp;&nbsp;
and correcting itself when it notices that&nbsp;
it's pursuing an unproductive direction.&nbsp;
How are we just like, "Oh yeah of course a&nbsp;
machine is gonna go think a bunch, come up&nbsp;&nbsp;
with a bunch of ideas, and come back to me with&nbsp;
a smart answer. That’s just what machines do."&nbsp;
Part of the reason some people are too&nbsp;
pessimistic is that they haven’t played&nbsp;&nbsp;
around with the smartest models in&nbsp;
domains where they’re the most competent.&nbsp;
Giving Claude Code a vague spec and just&nbsp;
sitting around for 10 minutes while it&nbsp;&nbsp;
zero shots a working application is a wild&nbsp;
experience. How did it do that? You can talk&nbsp;&nbsp;
about circuits and the training distribution&nbsp;
and RL or whatever, but the most proximal,&nbsp;&nbsp;
concise, and accurate explanation is simply that&nbsp;
it’s powered by a baby general intelligence.&nbsp;
At this point, part of you has to&nbsp;
be thinking, "It’s actually working.&nbsp;
We’re making machines that are intelligent."
My probability distributions are super wide.&nbsp;
And I want to emphasize that I do&nbsp;
believe in probability distributions.&nbsp;
Which means that work to prepare for a&nbsp;
misaligned 2028 ASI still makes a ton of sense.&nbsp;
I think this is a totally plausible outcome.
But here are the timelines at which&nbsp;&nbsp;
I’d take a 50/50 bet.
An AI that can do taxes&nbsp;&nbsp;
end-to-end for my small business as well as&nbsp;
a competent general manager could in a week:&nbsp;&nbsp;
including chasing down all the receipts on&nbsp;
different websites, finding the missing pieces,&nbsp;&nbsp;
emailing back and forth with anyone who we need&nbsp;
to hassle for invoices, filling out the form,&nbsp;&nbsp;
and sending it to the IRS. This I'd say 2028. I&nbsp;
think we’re in the GPT 2 era for computer use.&nbsp;
But we have no pretraining corpus, and the&nbsp;
models are optimizing for a much sparser&nbsp;&nbsp;
reward over a much longer time horizon using&nbsp;
action primitives they’re unfamiliar with.&nbsp;
That being said, the base model is&nbsp;
already decently smart and might have&nbsp;&nbsp;
a good prior over computer use tasks,&nbsp;
plus there’s a lot more compute and AI&nbsp;&nbsp;
researchers in the world, so it might even out.
Preparing taxes for a small business feels like&nbsp;&nbsp;
for computer use what GPT 4 was for language.
It took 4 years to get from GPT 2 to GPT 4.&nbsp;
Just to clarify, I am not saying that&nbsp;
we won’t have really cool computer use&nbsp;&nbsp;
demos in 2026 and 2027.
GPT-3 was super cool,&nbsp;&nbsp;
but it was not that practically useful.
I’m saying that these models won’t be capable&nbsp;&nbsp;
of end-to-end handling a week long and quite&nbsp;
involved project which involves computer use.&nbsp;
Ok, and as for the forecast of when AI&nbsp;
will be able to learn on the job as easily,&nbsp;&nbsp;
organically, seamlessly, and quickly&nbsp;
as humans, for any white collar work.&nbsp;
For example, if I hired an AI video editor, after&nbsp;
six months it would have as much actionable,&nbsp;&nbsp;
deep understanding of my preferences,&nbsp;
our channel, what works for the audience,&nbsp;&nbsp;
as well as a human would.
I'd say this would come in 2032.&nbsp;
While I don’t see an obvious way to&nbsp;
slot in continuous online learning&nbsp;&nbsp;
into the kinds of models these LLMs&nbsp;
are, 7 years is a really long time!&nbsp;
GPT 1 had just come out this time 7 years ago.
It doesn’t seem implausible to me that over the&nbsp;&nbsp;
next 7 years, we’ll find some way to get&nbsp;
these models to actually learn on the job.&nbsp;
At this point you might be reacting,&nbsp;
"Wait you made this huge fuss about&nbsp;&nbsp;
continual learning being such a huge handicap.
But then your prediction is that we’re 7 years&nbsp;&nbsp;
away from what, at a minimum, looks like a broadly&nbsp;
deployed intelligence explosion." And yeah, you’re&nbsp;&nbsp;
right. I am forecasting a pretty wild world within&nbsp;
a relatively short amount of time. AGI timelines&nbsp;&nbsp;
are very lognormal. It's either this decade or&nbsp;
bust. (Not really, it's more like lower marginal&nbsp;&nbsp;
probability per year—but that’s less catchy).
AI progress over the last decade has&nbsp;&nbsp;
been driven by scaling training&nbsp;
compute on the frontier systems.&nbsp;
It's been over 4x a year.
This cannot continue beyond this decade,&nbsp;&nbsp;
whether you look at chips, power, or even the&nbsp;
raw fraction of GDP that is used on training.&nbsp;
After 2030, AI progress has to mostly&nbsp;
come from algorithmic progress.&nbsp;
But even there all the low&nbsp;
hanging fruit will be plucked,&nbsp;&nbsp;
at least under the deep learning paradigm.
So the yearly probability of AGI collapses.&nbsp;
This means that if we end up on the longer side of&nbsp;
my 50/50 bets, we might be looking at a relatively&nbsp;&nbsp;
normal world up till the 2030s or even the 2040s.
But in all the other worlds, even if we stay sober&nbsp;&nbsp;
about the current limitations of AI, we&nbsp;
have to expect some truly crazy outcomes.&nbsp;
This was originally a blog post that I&nbsp;
published on my website at dwarkesh.com.&nbsp;
And it was obviously inspired by the discussion&nbsp;
I had with Sholto and Trenton on my podcast,&nbsp;&nbsp;
where I ended up disagreeing with them about&nbsp;
timelines but it took me a few weeks of thinking&nbsp;&nbsp;
afterwards, sorting out exactly where I&nbsp;
disagree and why I had longer timelines.&nbsp;
And I do this for other episodes as well.
I wrote up some thoughts I had about the many&nbsp;&nbsp;
thousands of pages that Stephen Kotkin has written&nbsp;
about Stalin, obviously which we were not able to&nbsp;&nbsp;
exhaustively cover in that one 2-hour interview.
So anyways, if you want to see these additional&nbsp;&nbsp;
artifacts and writing that I produce as a result&nbsp;
of this podcast and in preparation for episodes,&nbsp;&nbsp;
you should subscribe to my blog and newsletter.
You can do that at dwarkesh.com.&nbsp;
Otherwise I will also see you next week&nbsp;
for a full episode with a real guest.